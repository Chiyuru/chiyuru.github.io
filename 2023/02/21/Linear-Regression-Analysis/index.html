<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"chiyuru.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本来真没打算连载这个，结果这课居然把课程笔记算成百分制里面的 10 分（，那就写罢。 有参考 Vica Yang 的统辅笔记，JhZhang 的课堂笔记和 V1ncent19 的统辅笔记，如有引用均会在文中注明，在此向前辈表示感谢。">
<meta property="og:type" content="article">
<meta property="og:title" content="还就那个线性回归分析">
<meta property="og:url" content="https://chiyuru.github.io/2023/02/21/Linear-Regression-Analysis/index.html">
<meta property="og:site_name" content="『姑妄言之姑妄听之』">
<meta property="og:description" content="本来真没打算连载这个，结果这课居然把课程笔记算成百分制里面的 10 分（，那就写罢。 有参考 Vica Yang 的统辅笔记，JhZhang 的课堂笔记和 V1ncent19 的统辅笔记，如有引用均会在文中注明，在此向前辈表示感谢。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/02/26/AHxRkaSBD87fW3V.png">
<meta property="og:image" content="https://s2.loli.net/2023/02/26/iRyejXI6TuSvroJ.png">
<meta property="og:image" content="https://s2.loli.net/2023/03/16/XO3Bt8FehprNz5q.png">
<meta property="og:image" content="https://s2.loli.net/2023/03/16/3tpYiuFSd7Xw2vK.png">
<meta property="og:image" content="https://s2.loli.net/2023/05/29/VtZpD4vuSkA27eU.png">
<meta property="og:image" content="https://s2.loli.net/2023/05/29/iQKNWpv8IseAoFj.png">
<meta property="og:image" content="https://s2.loli.net/2023/05/29/Uy8JvP4eBmWS1GT.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/01/WTrmCsZ1oyLFzpQ.png">
<meta property="og:image" content="https://s2.loli.net/2023/05/29/VwIlizq5vsEFot6.png">
<meta property="og:image" content="https://s2.loli.net/2023/05/29/OJZi635KwUQ8quf.png">
<meta property="og:image" content="https://s2.loli.net/2023/05/29/BkYA4IrhbLXjTna.png">
<meta property="og:image" content="https://s2.loli.net/2023/05/30/KrAcZO2aIPQvYlS.png">
<meta property="og:image" content="https://s2.loli.net/2023/05/30/4hFWcryX6lbspt5.png">
<meta property="og:image" content="https://s2.loli.net/2023/05/31/g3Abuh529lnpV7r.png">
<meta property="og:image" content="https://s2.loli.net/2023/05/31/iNHqIePg5rVKTQM.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/02/Hb1oPSrA2IJFcmC.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/02/tjVnAgmPvwBoeQF.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/02/bAZXemT5kDncjoy.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/02/RP89DuvkNQlVUqi.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/02/McpX95nUsHYWRmT.png">
<meta property="og:image" content="https://s2.loli.net/2022/12/21/6TEM1vSosXLcwOg.jpg">
<meta property="article:published_time" content="2023-02-21T11:12:18.000Z">
<meta property="article:modified_time" content="2023-06-06T17:45:26.080Z">
<meta property="article:author" content="驰雨Chiyuru">
<meta property="article:tag" content="课程实录">
<meta property="article:tag" content="数学">
<meta property="article:tag" content="统计">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/02/26/AHxRkaSBD87fW3V.png">

<link rel="canonical" href="https://chiyuru.github.io/2023/02/21/Linear-Regression-Analysis/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>还就那个线性回归分析 | 『姑妄言之姑妄听之』</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="『姑妄言之姑妄听之』" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">『姑妄言之姑妄听之』</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-link">

    <a href="/links/" rel="section"><i class="fa fa-link fa-fw"></i>链接</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chiyuru.github.io/2023/02/21/Linear-Regression-Analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="驰雨Chiyuru">
      <meta itemprop="description" content="おはよう、朝だよ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="『姑妄言之姑妄听之』">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          还就那个线性回归分析
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-02-21 19:12:18" itemprop="dateCreated datePublished" datetime="2023-02-21T19:12:18+08:00">2023-02-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-06-07 01:45:26" itemprop="dateModified" datetime="2023-06-07T01:45:26+08:00">2023-06-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本来真没打算连载这个，结果这课居然把课程笔记算成百分制里面的 10
分（，那就写罢。</p>
<p>有参考 <a target="_blank" rel="noopener" href="https://blog.vicayang.cc/tags/statistics/">Vica Yang
的统辅笔记</a>，<a target="_blank" rel="noopener" href="https://www.zhihu.com/column/c_1332050772843511808">JhZhang
的课堂笔记</a>和 <a target="_blank" rel="noopener" href="https://v1ncent19.github.io/SummaryNotes/">V1ncent19
的统辅笔记</a>，如有引用均会在文中注明，在此向前辈表示感谢。</p>
<span id="more"></span>
<h1 id="lecture-1">Lecture 1</h1>
<p>主要是在吹水，开玩笑以及活跃气氛。正经的内容大概就一个古老的回归现象，我还没有听得很懂，麻了。</p>
<h2 id="galtons-experiment">Galton's Experiment</h2>
<p>以下全是胡说八道，不能保证完全对。</p>
<p>介绍正态分布的时候会有一个很经典的小球过钉板的演示实验，最后落在底部的球似乎呈现出一个正态分布。但实际上球和钉子的每次碰撞都是一个
Bernoulli 过程，过了 n 层钉板就是 n 次 Bernoulli
过程加和，可以近似为正态分布。实际上只要 n
够大，由中心极限定理任何分布的加和都可以被近似为正态分布。</p>
<p>但我觉得实际上小球的情况并不是独立的，毕竟过程中会有相互的碰撞，真的没问题吗（</p>
<p>回归现象的起源是 Galton
对于父代和子代的身高做了一个统计，发现身高远离均值的父母的后代往往身高会比他们更接近平均水平，也就是某一身高水平的父母的孩子的身高中位数作为因变量，父母身高作为自变量时，拟合出的直线的斜率小于
<span class="math inline">\(1\)</span>。</p>
<p>假设不发生回归现象，则和钉板现象一样，后代的性状会逐渐分散，这被认为是一个种群稳定性状的方式。听起来很玄学，似乎也有一个稍微合理的生物学解释了，但我们希望从统计学的角度分析这件事，背后是存在数学规律的。</p>
<p><img src="https://s2.loli.net/2023/02/26/AHxRkaSBD87fW3V.png" alt="Galton.png"></p>
<p>上图是课程中反复出现的一张图，对此做了很多解释。最上面的正态分布是父代的身高情况（实际上是父母身高的加权和），通过一个“倾斜槽”之后的第二个正态分布展示了子代的身高情况，比父代更加靠近中心。下方也有一个类似于钉板的装置，n
层钉板指的就是 n
代繁衍的过程，或者也可以指代一代繁衍中的其他影响身高的非基因因素，它们被视作独立同分布的，因此加和由中心极限定理可以被近似为正态分布。</p>
<p>图中还呈现出了父代中的一个小组“过钉板”后的结果，是一个小的正态分布。实际上子代的数据就是由一个一个小的正态分布叠加起来的，而正态分布可以线性相加，所以最后呈现出的还是正态分布。</p>
<p>好玄学，我也不知道我在说什么，甚至不是很确定自己理解对了没有。</p>
<p>Anyway，还是 think mathematically，记 <span class="math inline">\(X_i\)</span> 为第 <span class="math inline">\(i\)</span> 代的种群身高变化量，<span class="math inline">\(\lbrace X_i \rbrace\)</span>
是独立同分布的。则记父代种群的随机变量为 <span class="math inline">\(F=
\Sigma _{i=1} ^n X_i\)</span>，子代种群的随机变量为 <span class="math inline">\(S=
\Sigma_{i=1}^nX_i\)</span>，考虑二者的相关系数：</p>
<p><span class="math inline">\(\rho(F,S) =
\frac{cov(F,S)}{\sqrt{Var(F)Var(S)} } = \frac{Var(F) + Cov
(F,X_n)}{\sqrt{Var(F)Var(S)} }
=1+\frac{cov(F,X_n)}{Var(F)}&lt;1\)</span>，因此 <span class="math inline">\(\rho(F,X_n)&lt;0\)</span> 也即二者呈负相关。</p>
<h2 id="一些术语">一些术语</h2>
<ul>
<li><p>一般来说，我们把 <span class="math inline">\(X\)</span> 作为
predictor/input/explanatory variable，把 <span class="math inline">\(Y\)</span> 作为 response/output/dependent
variable。</p></li>
<li><p><span class="math inline">\(Y \sim X\)</span> 被称为 simple
regression，一元总归是简单的。</p>
<p><span class="math inline">\(Y \sim X_1,X_2,...,X_p\)</span> 称为
multiple/multivariate regression，实际上这两者是不一样的。</p>
<p>如果每个回归式中有超过一个 <span class="math inline">\(X\)</span>
就称为 multiple regression，如果有多个 <span class="math inline">\(Y\)</span> 就称为 multivariate
regression，并且每个式子里只能有一个不同的 dependent variable。</p>
<p>还有叫做 multivariate multiple regression 的回归方法，也就是用多个
<span class="math inline">\(X\)</span> 来预测多个 <span class="math inline">\(Y\)</span> 的情况，每个 <span class="math inline">\(Y\)</span> 出现在不同的式子里。</p></li>
<li><p>一般来说 <span class="math inline">\(Y\)</span>
是连续型随机变量，<span class="math inline">\(X\)</span>
可以是连续型、离散型或者分类型随机变量。有一些名词，不抄了，简单列一下：</p>
<p><img src="https://s2.loli.net/2023/02/26/iRyejXI6TuSvroJ.png" alt="regression.png"></p></li>
</ul>
<h1 id="lecture-2">Lecture 2</h1>
<h2 id="simple-linear-regression">Simple Linear Regression</h2>
<h3 id="数据的组织和表示">数据的组织和表示</h3>
<p>Simple linear regression 的数据一般是二元数据对 <span class="math inline">\((X_i , Y_i)\)</span>，每一对数据称为一个
case。数据集记作 <span class="math inline">\((X_1,Y_1),...,(X_n,Y_n)\)</span>，其中 <span class="math inline">\(X_i\)</span> 是 i-th observed explanatory
variable，<span class="math inline">\(Y_i\)</span> 是 i-th observed
response variable。</p>
<h3 id="模型的表示">模型的表示</h3>
<p><span class="math inline">\(Y_i = \beta_0 + \beta_1 X_i + \varepsilon
_i\)</span>，<span class="math inline">\(\varepsilon _i\)</span> 被称为
random error term，其中 <span class="math inline">\(\beta_0,\beta_1\)</span>
是参数。为了简化模型便于操作，需要对 random error term 做一些假设：</p>
<ul>
<li>均值为 <span class="math inline">\(E(\varepsilon_i) =
0\)</span>，方差为 <span class="math inline">\(Var(\varepsilon _i) =
\sigma ^2\)</span>，注意 <span class="math inline">\(\sigma\)</span>
是一个未知常数，也视作参数；</li>
<li>不同的 <span class="math inline">\(\varepsilon _i\)</span> 和 <span class="math inline">\(\varepsilon _j\)</span>
是不相关的。注意此处不需要不独立。</li>
</ul>
<p>对于更强的模型，例如 simple linear regression model with normal
error，我们直接要求 <span class="math inline">\(\varepsilon_1,...,\varepsilon_n i.i.d. \sim
N(0,\sigma^2)\)</span>（因为正态分布的不相关性和独立性等价）。这是一个很广泛的假设，但有时正态假设是明显有问题的，需要修正。</p>
<p>事实上，在 linear regression model 中，<span class="math inline">\(X_i\)</span> 和 <span class="math inline">\(Y_i\)</span> 的地位是不对等的。<span class="math inline">\(X_i\)</span> 被视为不带随机性的常数，而 <span class="math inline">\(Y_i\)</span> 因为 <span class="math inline">\(\varepsilon_i\)</span> 的存在是一个随机变量，有
<span class="math inline">\(E(Y_i) = \beta_0 + \beta_1
X_i\)</span>，<span class="math inline">\(Var(Y_i)=Var(\varepsilon_i) =
\sigma^2\)</span>。因此，回归线可以视作 <span class="math inline">\((X_i, E(Y_i))\)</span> 连成的直线，而数据点 <span class="math inline">\((X_i,Y_i)\)</span> 分布在回归线附近。</p>
<p>特别地，在正态假设下， 有 <span class="math inline">\(Y_i \sim
N(\beta_0 + \beta_1 X_i,\sigma^2)\)</span>，<span class="math inline">\(i=1,2,...,n\)</span>。</p>
<h3 id="参数的意义和求算">参数的意义和求算</h3>
<p>斜率 <span class="math inline">\(\beta_1\)</span> 的意义为 <span class="math inline">\(X_i\)</span> 增加 1 单位时 <span class="math inline">\(Y_i\)</span> 的变化量；截距 <span class="math inline">\(\beta_0\)</span> 的意义分两种情况解释，当 <span class="math inline">\(X_i\)</span> 取值范围中有 <span class="math inline">\(0\)</span> 时即为 <span class="math inline">\(X_i=0\)</span> 时的平均响应 <span class="math inline">\(E(Y_i)\)</span>，否则截距没有意义。</p>
<h4 id="least-sum-of-square-方法">Least Sum of Square 方法</h4>
<p>求算最佳参数实际上就是求使得 sum of squared diff 最小的 <span class="math inline">\(\hat{\beta_0},\hat{\beta_1}\)</span>，从而得到
<span class="math inline">\(Y_i\)</span> 的估计值 $= + X_i $。于是有
<span class="math inline">\(\hat{\beta_0},\hat{\beta_1} = \arg \min
_{\beta_0,\beta_1} \Sigma(Y_i - \beta_0 - \beta_1X_i)^2=\arg \min
_{\beta_0,\beta_1} \Sigma_{i=1 } ^n e_i
^2\)</span>。求导即可简单地得出：</p>
<p><span class="math display">\[b_1 = \hat{\beta_1} = \frac{\Sigma_i
(X_i - \bar{X})(Y_i - \bar{Y})}{\Sigma _i (X_i-\bar{X})^2}\]</span></p>
<p><span class="math display">\[b_0 = \hat{\beta_0} = \bar{Y} - b_1
\bar{X}\]</span></p>
<p>记残差为 <span class="math inline">\(e_i = Y_i - \hat{Y_i}= Y_i -
\hat{\beta_0} - \hat{\beta_1} X_i=\beta_0 + \beta_1X_i +\varepsilon _i -
\hat{\beta_0} - \hat{\beta_1}X_i \approx \varepsilon _i\)</span> ，也即
<span class="math inline">\(e_i\)</span> 为某一组 observed data <span class="math inline">\(Y_1,Y_2,...,Y_n\)</span> 之下得到的残差，但绝非
<span class="math inline">\(\varepsilon _i\)</span>
本身。残差是观测到的确定值，而 <span class="math inline">\(\varepsilon
_i\)</span> 是随机变量。</p>
<p>事实上求导的过程蕴含以下结论：</p>
<p><span class="math display">\[\Sigma _{i=1} ^n e_i=0\]</span></p>
<p><span class="math display">\[\Sigma _{i=1} ^{n} X_i e_i =
0\]</span></p>
<p>以上二式可以看做对 <span class="math inline">\(e_i\)</span>
的线性约束，<span class="math inline">\(\lbrace e_i \rbrace\)</span>
的自由度是 <span class="math inline">\(n-2\)</span>，互相之间不是独立的，这也是和 <span class="math inline">\(\lbrace \varepsilon _i \rbrace\)</span>
的差别之一。由此还可以得到一些其他性质，例如回归线必过 <span class="math inline">\((\bar{X},\bar{Y})\)</span>，不在此一一列举。</p>
<p>以上即为参数 <span class="math inline">\(\beta_0,\beta_1\)</span>
的估计方法。下面再考虑参数 <span class="math inline">\(\sigma\)</span>
的估计，使用残差 <span class="math inline">\(e_1,...,e_n\)</span>
来考虑。</p>
<p>取 <span class="math inline">\(\hat{\sigma ^2} = \frac{\Sigma _{i=1}
^n e_i ^2}{n-2}\)</span>。这是因为 <span class="math inline">\(e_1,e_2,...,e_n\)</span> 的自由度为 <span class="math inline">\(df_E = n-2\)</span>，由此考虑 sum of square <span class="math inline">\(SSE= \Sigma_{i=1} ^n (Y_i - \hat{Y_i})^2=\Sigma
_{i=1} ^n e_i ^2\)</span>，定义 mean of squared errors <span class="math inline">\(MSE= \frac{SSE}{df_E} = \frac{\Sigma_{i=1}^n e_i
^2}{n-2}\)</span> 为 <span class="math inline">\(\hat{\sigma
^2}\)</span>。</p>
<h4 id="mle-方法">MLE 方法</h4>
<p>也可以用推断课上的 MLE 方法。实际上，我们想找到一个 <span class="math inline">\(\beta_0,\beta_1\)</span> 的最佳估计，还可以使用
MLE 方法进行估计。</p>
<p>在正态假设下，我们可以将 <span class="math inline">\(n\)</span>
组数据视作 <span class="math inline">\(n\)</span>
个互相独立的随机变量，取使得其likelihood function 最大的一组 <span class="math inline">\(\beta_0,\beta_1,\sigma^2\)</span>
作为估计量。likelihood function 即为 <span class="math inline">\(f(y_1,y_2,...,y_n) =
f_1(y_1)...f_n(y_n)\)</span>，得到的 estimator 中 <span class="math inline">\(\hat{\beta_0},\hat{\beta_1}\)</span> 与 least sum
of square 中得出的估计量相同，但 <span class="math inline">\(\hat{\sigma
^2} = \frac{\Sigma _{i=1} ^n e_i
^2}{n}\)</span>。注意这是一个有偏的估计量，而 least sum of square
得到的估计量是无偏的。</p>
<h2 id="推断复习">推断复习</h2>
<p>咕了。什么嘛，我推断学得还是可以的嘛（x</p>
<h1 id="lecture-3">Lecture 3</h1>
<h2 id="线性回归中的推断">线性回归中的推断</h2>
<p>回顾一下，无论是 OLS 方法还是 MLE 方法，我们得到的参数估计 <span class="math inline">\(b_0\)</span>,<span class="math inline">\(b_1\)</span> 都是相同的：</p>
<p><span class="math display">\[b_1 = \hat{\beta_1} = \frac{\Sigma_i
(X_i - \bar{X})(Y_i - \bar{Y})}{\Sigma _i (X_i-\bar{X})^2}\]</span></p>
<p><span class="math display">\[b_0 = \hat{\beta_0} = \bar{Y} - b_1
\bar{X}\]</span></p>
<p>通过简单的计算可以知道：</p>
<p><span class="math display">\[E(b_1) = \beta_1,Var(b_1) ={\sigma^2
\over S_{XX}},s^2(b_1) = \frac{s^2}{S_{XX}}\]</span>，</p>
<p>其中 <span class="math inline">\(S_{XX} = \Sigma_{i=1} ^n (X_i -
\bar{X})^2\)</span>, <span class="math inline">\(s^2 = \hat{\sigma}^2 =
\frac{\Sigma_{i=1}^n e_i^2}{n-2}\)</span>。</p>
<p>而且有 <span class="math inline">\(Cov(b_1, \bar Y)=0\)</span>。</p>
<h3 id="参数推断">参数推断</h3>
<ul>
<li><p>对 <span class="math inline">\(\beta_1\)</span> 进行推断：null
hypothesis 为 <span class="math inline">\(H_0: \beta_1
=0\)</span>，这样设置是因为关心两个变量之间是否存在线性关系。</p>
<p>在假设 <span class="math inline">\(H_0\)</span> 下可以考虑 test
statistic 为 <span class="math inline">\(T= \frac{b_1 -0}{s(b_1)} \sim
t_{n-2}\)</span>，这是因为 <span class="math inline">\(H_0\)</span>
假设下 <span class="math inline">\(b_1-0 \sim
N(0,\frac{\sigma}{\sqrt{Sxx}})\)</span>，我们一般用 <span class="math inline">\(\sigma\)</span> 的无偏估计 <span class="math inline">\(\hat{\sigma} = \sqrt{\frac{\Sigma_{i=1}^n e_i
^2}{n-2}}\)</span> 来处理。</p>
<p>level of significance 为 <span class="math inline">\(\alpha\)</span>，于是当 observed data <span class="math inline">\(t_0\)</span> 满足 <span class="math inline">\(|t_0| &gt; t_{n-2,1-\alpha /2}\)</span> 时 reject
<span class="math inline">\(H_0\)</span>。</p>
<p>如果没有拒绝 <span class="math inline">\(H_0\)</span>，通常的可能有以下三种：</p>
<ul>
<li>发生了 Type II Error，没有成功拒绝掉 <span class="math inline">\(H_0\)</span> 而事实相反；</li>
<li><span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 之间确实没有什么线性关系；</li>
<li><span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 之间有关系，但是非线性。（这句话原来的
typo 是“有线性关系，但是非线性”，绷不住了，中午被 Photon
指出来了，非常感谢他）</li>
</ul></li>
<li><p>对 <span class="math inline">\(\beta_1\)</span> 做 confidence
interval：由于在 <span class="math inline">\(\beta_1\)</span>
代表斜率的情况下，有 <span class="math inline">\(\frac{b_1 -
\beta_1}{s(b_1)} \sim^{H_0} t_{n-2}\)</span>。</p>
<p>于是 <span class="math inline">\(P(\frac{|b_1-\beta_1|}{s(b_1)} &lt;
t_{n-2,1-\alpha /2}) = 1-\alpha\)</span>，<span class="math inline">\(\beta_1\)</span> 的 100%<span class="math inline">\((1-\alpha)\)</span> confidence interval 是</p>
<p><span class="math display">\[(b_1 - t_{n-2,1-\alpha /2} s(b_1),b_1 +
t_{n-2,1-\alpha /2} s(b_1))\]</span></p></li>
<li><p>类似地可以对 <span class="math inline">\(\beta_0\)</span>
做推断，有 <span class="math inline">\(\frac{b_0 -\beta_0}{s(b_0)} \sim
t_{n-2}\)</span>，因此如果 null hypothesis 为 <span class="math inline">\(H_0: \beta_0 =0\)</span>，rejection region
即为满足条件 $ &gt; t_{n-2,1-/2} $ 的数据。对于偏移的 null hypothesis
<span class="math inline">\(H_0 : \beta_0 =c\)</span>，也只要相应地移动
rejection region 即可。</p>
<p>100%<span class="math inline">\((1-\alpha)\)</span> confidence
interval 为 <span class="math inline">\((b_0 - t_{n-2,1-\alpha /2}
s(b_0),b_0 + t_{n-2,1-\alpha /2} s(b_0))\)</span>。</p>
<p>实际上我们一般对 <span class="math inline">\(\beta_0\)</span>
的推断不感兴趣，因为这个参数未必有意义，依赖于 <span class="math inline">\(X\)</span> 的取值范围。</p></li>
<li><p>以上都是对单个参数进行推断，实际上我们也可以进行 joint
inference：</p>
<p>同时推断两个参数 <span class="math inline">\((\beta_0,\beta_1)\)</span>，这时候得到的就是
confidence region，使得 <span class="math inline">\(P((\beta_0,\beta_1)
\in S \subset \mathbb R^2) = 100(1-\alpha) \%\)</span>。实际上因为 <span class="math inline">\((b_0 , b_1) ^T \sim N((\beta_0,\beta_1)^T ,
\sigma^2 \Sigma_{2\times 2})\)</span>，所以最小的 confidence region
是一个椭圆。</p>
<p>实际上我们也可以考虑做一个矩形的 confidence
region，也即对两个参数分别作 confidence interval，confidence coefficient
分别为 <span class="math inline">\(\sqrt{1-\alpha} \approx
1-\frac{\alpha}{2}\)</span>。因此 confidence region 为：</p>
<p><span class="math display">\[ (b_1 - t_{n-2,1-\alpha /4} s(b_1),b_1 +
t_{n-2,1-\alpha /4} s(b_1)) \times (b_0 - t_{n-2,1-\alpha /4} s(b_0),b_0
+ t_{n-2,1-\alpha /4} s(b_0))\]</span></p></li>
</ul>
<p>参数推断的角度来说 <span class="math inline">\(\beta_1\)</span>
的推断远比 <span class="math inline">\(\beta_0\)</span>
重要，它表征线性关系，而且从预测的角度来说，<span class="math inline">\(\beta_1\)</span> 的推断如果不够精细，会导致远离
<span class="math inline">\(\bar X\)</span> 处的 <span class="math inline">\(X_h\)</span> 对应的估计量误差很大。</p>
<h3 id="power-function">Power Function</h3>
<p>一个 significance test 的 power 指的是 reject <span class="math inline">\(H_0\)</span> 时 <span class="math inline">\(H_1\)</span> 是正确的的概率，也就是 <span class="math inline">\(1-P(\)</span>Type II Error<span class="math inline">\()\)</span>。实际上一个推断是好的的情况下需要既不
over-powered 也不 under powered，这和 Type I Error 与 Type II Error
此消彼长的性质有关。</p>
<p>Power function 一般是一个关于参数的函数。以推断 <span class="math inline">\(\beta_1\)</span> 的过程为例，计算这一推断的 power
function。</p>
<ul>
<li><p>我们在线性回归参数推断里会用到一种非中心化 t-分布。普通的
t-分布是关于 <span class="math inline">\(x=0\)</span> 对称的，非中心化
t-分布有一定的偏差。注意并不是整体在坐标轴方向上的移动，其形状也发生了变化。表达式为
<span class="math inline">\(t(df, \delta ) = \frac{N(0,1) +
\delta}{\sqrt{\chi_{df} / df}}\)</span>。</p></li>
<li><p>对 <span class="math inline">\(\beta_1\)</span> 进行推断：null
hypothesis 为 <span class="math inline">\(H_0: \beta_1
=0\)</span>，这样设置是因为关心两个变量之间是否存在线性关系。</p>
<p>在假设 <span class="math inline">\(H_0\)</span> 下可以考虑 test
statistic 为 <span class="math inline">\(T= \frac{b_1 -0}{s(b_1)} \sim
t_{n-2}\)</span>，然而在 <span class="math inline">\(H_1\)</span>
下,</p>
<p><span class="math display">\[T=\frac{b_1}{s(b_1)} = \frac{b_1 -
\beta_1 + \beta_1}{s(b_1)} = \frac{b_1-\beta_1}{s(b_1)} + \frac{\beta_1
/ \sigma(b_1)}{s(b_1)/\sigma(b_1)} =  \frac{N(0,1) +
\delta}{\sqrt{\chi_{df} / df}} \sim
t(n-2,\beta_1/\sigma(b_1))\]</span></p>
<p>于是 <span class="math inline">\(Power(\beta_1) = P(Reject H_0 | H_1
holds) = P(|T| &gt; t_{n-2,1-\alpha /2} | \beta_1 \neq 0) =
P(T&lt;t_{n-2,\alpha /2}) +1-P(T&lt;t_{n-2,1-\alpha
/2})\)</span>，其中在 <span class="math inline">\(\beta_1 \neq
0\)</span> 的条件下，<span class="math inline">\(T\sim t(n-2,\beta_1 /
\sigma(b_1))\)</span>。</p></li>
</ul>
<h2 id="blue">BLUE</h2>
<p><a target="_blank" rel="noopener" href="https://record.umich.edu/articles/origins-go-blue/">Go Blue
!</a>（</p>
<p>简单来说，OLS Estimators 是 the best linear unbiased estimator，简称
OLS estimators 是 BLUE。best 的意思是方差最小，这是不难证明的。</p>
<p>BLUE 是非常好的性质，也希望我以后能 go blue（逃</p>
<h1 id="lecture-4">Lecture 4</h1>
<p>Prediction &amp; ANOVA，感谢 zzy 救我的生统概论（</p>
<h2 id="estimation-prediction">Estimation &amp; Prediction</h2>
<h3 id="平均响应的推断">平均响应的推断</h3>
<p>有了线性回归模型之后当然是要用来做预测，通过已有数据拟合出一个线性模型，再用来估计未知点的值。对于需要估计的点
<span class="math inline">\(X_h\)</span>，一般来说估计值都是考虑平均响应
(mean response) <span class="math inline">\(\mu_h = E(Y_h) = \beta_0
+\beta_1 X_h\)</span>，把 <span class="math inline">\(\hat{\mu_h} = b_0
+ b_1 X_h\)</span> 作为 <span class="math inline">\(\mu_h\)</span>
的估计。这是一个
estimator，既然如此就要考虑它的性质，也要先考虑平均响应的置信区间。</p>
<ul>
<li><p><span class="math inline">\(E(\hat{\mu_h}) = E(b_0) + E(b_1)X_h =
\beta_0 + \beta_1 X_h = \mu_h\)</span> ，是 unbiased estimator</p></li>
<li><p><span class="math inline">\(Var(\hat{\mu_h}) = Var(\bar{Y}+
b_1(X_h - \bar{X})) = \sigma^2 [\frac{1}{n} + \frac{(X_h -
\bar{X})^2}{\Sigma (X_i -\bar{X})^2}]\)</span> ，是 minimum
variance</p></li>
<li><p><span class="math inline">\(\hat{\mu_h} = \bar{Y} + (X_h -
\bar{X}) b_1\)</span>，由于 <span class="math inline">\(\bar{Y}\)</span>
和 <span class="math inline">\(b_1\)</span> 都有正态假设，因此 <span class="math inline">\(\hat{\mu_h}\)</span> 也服从正态分布，<span class="math inline">\(\hat{\mu_h} \sim N(\mu_h ,
Var(\hat{\mu_h}))\)</span>。</p></li>
<li><p><span class="math inline">\(Var(\hat{\mu_h})\)</span> 的估计量是
<span class="math inline">\(s^2(\hat{\mu_h}) = s^2[\frac 1 n +
\frac{(X_h-\bar{X})^2}{\Sigma(X_i - \bar{X})^2}]\)</span>，于是有 <span class="math inline">\(\frac{\hat{\mu_h} - \mu_h}{s(\hat{\mu_h})} \sim
t_{n-2}\)</span>。</p>
<p>因此，<span class="math inline">\(\mu_h\)</span> 的 <span class="math inline">\(100 \% (1-\alpha)\)</span> confidence interval 是
<span class="math inline">\((\hat{\mu_h} - t_{n-2, 1-\alpha /2}
s(\hat{\mu_h}),\hat{\mu_h} + t_{n-2, 1-\alpha /2}
s(\hat{\mu_h}))\)</span>。confidence interval 的长度为 $2t_{n-2, 1-/2}
s() = 2t_{n-2, 1-/2} s $，其中 <span class="math inline">\(s =
\sqrt{[\frac{\Sigma_{i=1}^n
e_i^2}{n-2}]}\)</span>。因此置信区间的长度是近似于随 <span class="math inline">\(X_h - \bar{X}\)</span>
递增而递增的。<strong>也就是说，<span class="math inline">\(X_h\)</span>
距离 <span class="math inline">\(\bar{X}\)</span>
越远，置信区间的长度越大，准确性越难保证。</strong></p>
<p>综上，我们成功找到了这个对于 <span class="math inline">\(\mu_h\)</span> 的估计的置信区间。</p></li>
</ul>
<h3 id="预测值的推断">预测值的推断</h3>
<p>上述估计的是预测值的平均响应，对于新观测点的值需要改成：<span class="math inline">\(Y_{h(new)} = \beta_0 + \beta_1 X_h
+\varepsilon\)</span> 且有 <span class="math inline">\(E(\varepsilon) =
0,Var(\varepsilon) = \sigma^2\)</span>。注意这里 <span class="math inline">\(\varepsilon\)</span> 是随机变量，<span class="math inline">\(\beta_0,\beta_1\)</span> 是未知值的参数，<span class="math inline">\(X_h\)</span> 是已知的常数。</p>
<p>对它做估计 <span class="math inline">\(\hat{Y}_{h(new)} = \hat{\mu_h}
= b_0+b_1 X_h\)</span> 仍然是和平均响应相同，考虑这个估计的性质。</p>
<ul>
<li><p><span class="math inline">\(E(\hat{Y}_{h(new)}) = E(b_0 + b_1
X_h) = \beta_0 + \beta_1 X_h\)</span></p></li>
<li><p>从简单的情形开始，如果 <span class="math inline">\(\beta_0,\beta_1,\sigma\)</span>
都是已知的参数，则在正态假设下 <span class="math inline">\(\frac{Y_{h(new)} -E(Y_h)}{\sigma}\sim
N(0,1)\)</span>（非正态假设情况下服从未知分布），<span class="math inline">\(Y_{h(new)}\)</span> 的 confidence interval 是
<span class="math inline">\((E(Y_h) - z_{1-\alpha /2} \sigma,E(Y_h) +
z_{1-\alpha /2} \sigma)\)</span>。</p></li>
<li><p>一般情况下，设 <span class="math inline">\(d_h = Y_{h(new)} -
\hat{Y}_{h(new)} = Y_{h(new)} - \hat{\mu_h}\)</span>，于是有 <span class="math inline">\(E(d_h) = 0\)</span>。</p>
<p>计算可知方差 <span class="math inline">\(Var(d_h) = Var(Y_{h(new)} -
\hat{\mu} _h) = Var(Y_{h(new)})+Var(\hat{\mu_h}) = \sigma^2[1+\frac 1 n
+\frac{(X_h - \bar{X})^2}{\Sigma(X_i - \bar{X})^2}]\)</span>；</p>
<p>standard error 为 <span class="math inline">\(s^2(d_h) = s^2 [1+\frac
1 n +\frac{(X_h - \bar{X})^2}{\Sigma(X_i - \bar{X})^2}]\)</span>；</p>
<p>于是有 <span class="math inline">\(\frac{d_h - E(d_h)}{s(d_h)} =
\frac{d_h}{s(d_h)} = \frac{Y_{h(new)} - \hat{\mu}_h}{s(d_h)} \sim
t_{n-2}\)</span>，<span class="math inline">\(Y_{h(new)}\)</span>
的置信区间是 <span class="math inline">\((\hat{\mu}_h - s(d_h)
t_{n-2,1-\alpha/2},\hat{\mu}_h + s(d_h)
t_{n-2,1-\alpha/2})\)</span>，这个区间一般叫做 prediction
interval，长度是 <span class="math inline">\(2 t_{n-2,1-\alpha/2}
s(d_h)\)</span>，其中 <span class="math inline">\(s^2(d_h) = s^2 +
s^2(\hat{\mu}_h
^2)\)</span>，因此<strong>预测区间比平均响应的置信区间略宽</strong>。</p></li>
</ul>
<h3 id="预测值的平均的推断">预测值的平均的推断</h3>
<p>考虑在新值 <span class="math inline">\(X_h\)</span> 处的 <span class="math inline">\(m\)</span> 个观测值的平均值，为 ${Y_h} = m
<em>{i=1} ^m Y</em>{h(new) i} = _0 +<em>1 X_h+ 1m </em>{i=1}^m _i
$，预测值的平均仍然是 <span class="math inline">\(\hat{Y_h}=b_0 + b_1
X_h\)</span>。有 <span class="math inline">\(\varepsilon _i \sim
N(0,\sigma^2)\)</span> 为正态假设。于是 <span class="math inline">\(Var(\bar{Y}_h - \hat{Y_h}) = \sigma^2 [\frac 1m +
\frac 1 n + \frac{(X_h - \bar{X})^2}{\Sigma(X_i - \bar{X})^2}] \leq
Var(d_h)\)</span>。</p>
<p>它的宽度小于 prediction interval，但也大于平均响应的 confidence
interval。</p>
<h3 id="confidence-band-for-entire-regression-line">Confidence Band for
Entire Regression Line</h3>
<p>怎么翻译都没那味，就写原文吧。</p>
<p>希望找到一个 confidence band：<span class="math inline">\(\lbrace
(x,y): L(x)&lt;y&lt;U(x), x\in \mathbb R \rbrace\)</span>，对于任意点
<span class="math inline">\(x\)</span> 有 <span class="math inline">\(P[l(x) &lt; \beta_0 +\beta_1 x &lt;u(x)] =
1-\alpha\)</span>。对于固定的点 <span class="math inline">\(x\)</span>，<span class="math inline">\(100 \%
(1-\alpha)\)</span> 置信区间为 <span class="math inline">\((\hat{\mu_x}
- t_{n-2, 1-\alpha /2} s(\hat{\mu_x}),\hat{\mu_x} + t_{n-2, 1-\alpha /2}
s(\hat{\mu_x}))\)</span>。</p>
<p>所以只要取 <span class="math inline">\(W=max\{(\hat{\mu}_x -
\mu_x)/s(\hat{\mu}_x) \}\)</span>，即有置信区间为 <span class="math inline">\((\hat{\mu}_x - W s(\hat{\mu_x}),\hat{\mu}_x + W
s(\hat{\mu_x}))\)</span>，其中 <span class="math inline">\(W =
\sqrt{2F_{1-\alpha,2,n-2}}\)</span>。实际上 <span class="math inline">\(W &gt; t_{n-2,1-\alpha /2}\)</span>，也就是说
confidence band 处处比 confidence interval 更宽，level of confidence
<span class="math inline">\(\alpha&#39;\)</span> 也更小。</p>
<h3 id="summary">Summary</h3>
<p>一个显示 confidence interval of mean response，prediction interval 和
confidence band 宽度关系的图：</p>
<p><img src="https://s2.loli.net/2023/03/16/XO3Bt8FehprNz5q.png" alt="bands.png"></p>
<h2 id="analysis-of-variance-anova">Analysis of Variance (ANOVA)</h2>
<h3 id="variance-estimator">Variance Estimator</h3>
<p>先上点概念：</p>
<ul>
<li><p>Total Sum of Squares: <span class="math inline">\(SST= \Sigma(Y_i
- \bar{Y})^2\)</span>，<span class="math inline">\(df_{SST} =
n-1\)</span></p>
<p>Sample Variance: <span class="math inline">\(S_n ^2 =
\frac{SST}{n-1}\)</span>，是非常熟悉的统计量。</p></li>
<li><p>Variation due to Error: <span class="math inline">\(SSE =
\Sigma(Y_i -\hat{Y}_i)^2= \Sigma e_i ^2\)</span>, <span class="math inline">\(df_{SSE} = n-2\)</span></p>
<p>Mean Square Error: <span class="math inline">\(MSE = \frac{SSE}{df} =
\frac{\Sigma_{i=1} ^n e_i ^2}{n-2}\)</span>，可以作为 <span class="math inline">\(\sigma^2 = Var(\varepsilon)\)</span>
的一个估计。</p>
<p><span class="math inline">\(E(MSE) = \sigma^2\)</span></p></li>
<li><p>Variation due to Regression: <span class="math inline">\(SSR =
\Sigma(\hat{Y}_i - \bar{Y})^2 = b_1 ^2 \Sigma (X_i-\bar{X})^2\)</span>,
<span class="math inline">\(df_{SSR} = 1\)</span></p>
<p>Mean Squares of Regression: <span class="math inline">\(MSR =
\frac{SSR}{df} = SSR\)</span></p>
<p>$E(MSR) = E(SSR) = E(b_1 ^2) ((X_i - {X})^2)= ^2 + _1^2 (X_i - {X})^2
$;</p></li>
</ul>
<p>可以计算得到 <span class="math inline">\(SST=SSE+SSR\)</span>，自由度方面也是 <span class="math inline">\(df_{SST} = df_{SSE}+df_{SSR}\)</span>。</p>
<p>事实上在 Lecture 2 中我们考虑过是选取 sample variance 还是选取 MSE
作为 <span class="math inline">\(\hat{\sigma}^2\)</span>，这里的定义给出了更清晰的答案：<span class="math inline">\(SSE\)</span> 更加注重原值和模型之间的联系，把
<span class="math inline">\(\hat{Y_i}\)</span> 作为 <span class="math inline">\(Y_i\)</span> 的估计值，在 sample variance 中是把
<span class="math inline">\(\bar{Y}\)</span> 作为 <span class="math inline">\(Y_i\)</span> 的估计值，前者更好地体现了 <span class="math inline">\(\varepsilon\)</span> 的场景。</p>
<h3 id="f-检验">F-检验</h3>
<p>我们希望通过以上统计量检验 <span class="math inline">\(X,Y\)</span>
之间是否存在线性关系。考虑假设 <span class="math inline">\(H_0: \beta_1
= 0 ; H_1 : \beta_1 \neq 0\)</span>。</p>
<p>在正态假设和 <span class="math inline">\(H_0\)</span> 成立条件下，取
test statistic 为 <span class="math inline">\(F= \frac{MSR}{MSE} =
\frac{SSR / df_R}{SSE / df_E} = \frac{\chi_{dfR}/df_R}{\chi_{dfE}/df_E}
\sim F_{df_R,df_E} = F_{1,n-2}\)</span>。考虑拒绝 <span class="math inline">\(H_0\)</span> 的情况，我们要求 <span class="math inline">\(F_0 &gt; F_{1-\alpha,df_R,df_E} = F_{1-\alpha,
1,n-2}\)</span>，满足此条件的 <span class="math inline">\((X,Y)\)</span>
构成 rejection region。或者从 P-value 的角度来看，满足不等式 <span class="math inline">\(P(F&gt;F_0 | F_{1,n-2}) &lt; \alpha\)</span>
的全体 <span class="math inline">\((X,Y)\)</span> 落在 rejection region
中。</p>
<p>考虑检验的 power function。在 <span class="math inline">\(H_1:
\beta_1 \neq 0\)</span> 条件下，Power(<span class="math inline">\(\beta_1\)</span>) = <span class="math inline">\(P(F&gt; F_{1-\alpha , 1,n-2} | non-central
F)\)</span>。</p>
<h3 id="general-linear-test">General Linear Test</h3>
<p>另一种检验上述假设的方式。考虑假设 <span class="math inline">\(H_0:
\beta_1 = 0 ; H_1 : \beta_1 \neq
0\)</span>，我们来比较两种不同的模型：</p>
<p>full model: <span class="math inline">\(Y_i = \beta_0+\beta_1 X_i
+\varepsilon _i\)</span>，reduced model: <span class="math inline">\(Y_i
= \beta_0 + \varepsilon_i\)</span>，在 <span class="math inline">\(H_0\)</span>
假设成立时两个模型是等价的。考虑方差统计量：</p>
<ul>
<li><span class="math inline">\(SSE(F) = SSE\)</span> for full model
with <span class="math inline">\(df_{EF}\)</span>，在 simple linear
regression 下 <span class="math inline">\(df_{EF}=n-2\)</span>，<span class="math inline">\(SSE(F) = SSE\)</span></li>
<li><span class="math inline">\(SSE(R) = SSE\)</span> for reduced model
with <span class="math inline">\(df_{ER}\)</span>，在 simple linear
regression 下 <span class="math inline">\(df_{ER}=n-1\)</span>，<span class="math inline">\(SSE(R) = SST\)</span></li>
</ul>
<p>在 <span class="math inline">\(H_0\)</span> 假设下，$F =
F_{(df_{ER}-df_{EF}),df_{EF}} $。实际上，在 simple linear regression
下和 F-检验是一致的。但是 general linear test
是一个更广泛的方法，可以用于任何形式的线性检验。</p>
<h3 id="pearson-correlation-r">Pearson Correlation r</h3>
<p><span class="math inline">\(r = \frac{\Sigma (X_i - \bar{X})(Y_i -
\bar{Y})}{\sqrt{\Sigma(X_i - \bar{X})^2} \sqrt{\Sigma(Y_i -
\bar{Y})^2}}\)</span> 被称为相关系数，反映了一组数据 <span class="math inline">\((X,Y)\)</span> 之间的线性关系程度。</p>
<p><span class="math inline">\(R^2 = \frac{SSR}{SST}\)</span>
被称为决定系数，事实上在 simple linear regression 下有 <span class="math inline">\(r^2=R^2\)</span>，因为 <span class="math inline">\(r = b_1 \frac{\sigma_X}{\sigma_Y}\)</span>。</p>
<p><img src="https://s2.loli.net/2023/03/16/3tpYiuFSd7Xw2vK.png" alt="2dimtest.png"></p>
<h1 id="lecture-5">Lecture 5</h1>
<p>回顾一下简单线性回归的模型假设：<span class="math inline">\(Y_i =
\beta_1 X_i + \beta_0 +\varepsilon _i\)</span></p>
<ul>
<li><span class="math inline">\(\varepsilon _1, \varepsilon _2 ,...,
\varepsilon_n\)</span> 相互独立</li>
<li><span class="math inline">\(\varepsilon_i\)</span> 服从正态分布</li>
<li><span class="math inline">\(\mathbb E(\varepsilon _i) =
0\)</span>，<span class="math inline">\(Var(\varepsilon_i) =
\sigma^2\)</span></li>
</ul>
<p>可以总结成 LINE: linearity, independence, normality, equal
variance</p>
<h2 id="diagnostics-of-x">Diagnostics of X</h2>
<p>诊断的方式粗暴一点来说就是肉眼诊断，用一些可视化工具（主要是画图）和其他方式来检验模型的假设是否符合。如果违反了模型假设，结果很有可能不可靠。此时需要用一些弥补的方式来处理。</p>
<h3 id="why-diagnosedistribution-and-confounding">Why
diagnose——Distribution and Confounding</h3>
<p>诊断过程需要关注的是 <span class="math inline">\(X\)</span> 而非
<span class="math inline">\(Y\)</span>，因为 <span class="math inline">\(Y\)</span> 之间是独立异分布的。<span class="math inline">\(Y_i \sim N(\beta_0 +\beta_1 X_i ,
\sigma^2)\)</span>。</p>
<p><span class="math inline">\(X\)</span> 完全是常数，所谓的 <span class="math inline">\(X\)</span> 的分布指的仅仅是 <span class="math inline">\(X_1,X_2,...,X_n\)</span>
在数轴上的排布，不是概率分布。但 <span class="math inline">\(X_i\)</span>
的分布会影响到模型的效果，直观上举个例子来说，<span class="math inline">\(X_i\)</span> 的位置至少会影响到 <span class="math inline">\(Var(b_1) = \frac{1}{\Sigma_i (X_i - \bar X)^2}
\sigma^2\)</span> 也就是 <span class="math inline">\(b_1\)</span>
的分散程度，这是会影响到推断显著性的因素，所以即使是常数也还是要对 <span class="math inline">\(X\)</span> 进行一些诊断。</p>
<p>我们一般希望 <span class="math inline">\(X\)</span>
是类似于正态的分布，这样的数据比较有代表性。可以使用 qq-plot
进行检查。</p>
<p>除此之外诊断 <span class="math inline">\(X\)</span> 的另一意义在于
<span class="math inline">\(X\)</span>
本身可能也是和其他因素有混杂的。举个例子来说，如果高温既会导致冰激凌销量增加又会导致鲨鱼攻击人的次数增加，很可能会发现冰激凌销量和鲨鱼攻击行为次数之间有线性的关系，事实上这就是一个没有选对合适的
explanatory variable <span class="math inline">\(X\)</span>
却得到了看似合理的模型的例子，但这样的结论是有问题的，<span class="math inline">\(X\)</span> 本身和气温这一因素混杂。</p>
<p>此时如果分别对冰激凌销量-气温和鲨鱼攻击行为-气温作一个 sequence
plot，会发现二者都分别和气温有关系，那就有必要把气温作为一个 explanatory
variable 加入模型的考虑，这是一种针对 confounding 的诊断方法。</p>
<h3 id="四参数">四参数</h3>
<p>有一些可以关心的量（甚至不能说是统计量，毕竟 <span class="math inline">\(X\)</span> 也不是随机变量），除了 range
之外实际上就是一阶到四阶矩：</p>
<ul>
<li>sample mean 展现了 <span class="math inline">\(X\)</span>
的主要位置</li>
<li>standard deviation 展现了数据的分散程度</li>
<li>偏度 skewness <span class="math inline">\(g_1 = \frac{m_3}{m_2
^\frac 3 2} = \frac{\frac 1 n \Sigma_{i=1}^n (x_i - \bar x)^3}{(\frac 1
n \Sigma_{i=1}^n (x_i -\bar x)^2)^\frac 3 2}\)</span>
展现了数据的对称性</li>
<li>峰度 kurtosis <span class="math inline">\(g_2 = \frac{m_4}{m_2 ^2}
-3 = \frac{\frac 1 n \Sigma_{i=1}^n (x_i - \bar x )^4}{(\frac 1 n
\Sigma_{i=1} ^n (x_i -\bar x)^2)^2} - 3\)</span>
展现了数据相对于正态分布的尾迹</li>
<li>range 展现了 <span class="math inline">\(X\)</span> 的分布范围</li>
</ul>
<p>其中值得关注的是偏度和峰度两个统计量，<del>因为之前没有提过</del>。想起来一个乐子，Pearson
在《Lady Tasting
Tea》里曾经认为一个分布只要有一阶到四阶矩的参数就可以完全确定，但实际上
Poisson 分布的四个参数都是 <span class="math inline">\(\lambda\)</span>，是不行的。</p>
<ul>
<li><p>关于偏度：</p>
<ul>
<li><span class="math inline">\(g_1 &lt;0\)</span> 时称为 negatively
skewed，左尾比较长，所以也会称为 skewed left</li>
<li><span class="math inline">\(g_1 &gt;0\)</span> 时称为 positively
skewed，右尾比较长，所以也会称为 skewed right</li>
</ul>
<p>由图可见偏度是能够体现数据的对称程度的。实际上对称程度是相对正态分布而言的。</p>
<p>一般来说对于一个左偏的分布，会有 mean &lt; median &lt;
mode（众数），如果右偏则会是 mode &lt; median &lt; mean。当然 mean 和
median 的顺序不一定准确，以及对于完全对称的分布会有三者相等。</p>
<p>注意如果分布有多个峰值，此时 skewness 不一定还适用。</p></li>
<li><p>关于峰度：</p>
<ul>
<li><span class="math inline">\(g_2&gt;0\)</span> 时称为
leptokurtic，尖峰态下双尾较长。</li>
<li><span class="math inline">\(g_2 &lt;0\)</span> 时称为
platykurtic，低峰态下双尾较短。</li>
</ul>
<p>注意峰度的所谓尖峰态低峰态和尾部数据性质也都是相对正态分布而言的。正态分布的峰度就是
<span class="math inline">\(3\)</span>，因此峰度的公式里有一个减去 <span class="math inline">\(3\)</span> 的操作，作为和正态分布的比对。</p>
<p>有的时候会把不减去 <span class="math inline">\(3\)</span> 的称为
kurtosis，减去 <span class="math inline">\(3\)</span> 则称为 excess
kurtosis，使用的时候要注意。</p>
<p>实际上峰度带来的度量信息包括峰和尾两部分，单独出现尖峰的条件不能作为判断
<span class="math inline">\(g_2\)</span> 正负性的依据，只是表征了 <span class="math inline">\(\bar X\)</span>
附近的情况，和尾部情况综合起来看才可以；峰度的正负性和方差的大小无关。</p>
<p>尾部的情况可以通过 Q-Q plot 查看：</p>
<p><img src="https://s2.loli.net/2023/05/29/VtZpD4vuSkA27eU.png" alt="kurtosis.png"></p></li>
</ul>
<h2 id="诊断-assumptions">诊断 assumptions</h2>
<p>最常用的 assumption
诊断方法是使用残差图进行诊断，一元线性回归中我们可以直接使用 <span class="math inline">\(e_i \sim X\)</span> 图，也可以使用 <span class="math inline">\(e_i \sim \hat Y\)</span>
图进行诊断，二者本质上只相差横轴的尺度和位置。在多元线性回归中就直接使用
<span class="math inline">\(e_i \sim \hat Y\)</span> 图进行诊断。</p>
<p>除此之外也有很多理论检验的方法，虽然听起来更
concrete，但其实实际应用中还是肉眼检查最有效。</p>
<p>模型诊断可能发现的一些问题：</p>
<ul>
<li><span class="math inline">\(Y\)</span> 和 <span class="math inline">\(X\)</span> 之间没有线性关系但是硬拟合了一个</li>
<li><span class="math inline">\(\sigma ^2\)</span>
不能视作常数，也即异方差</li>
<li><span class="math inline">\(\varepsilon _i\)</span>
不服从正态分布</li>
<li><span class="math inline">\(\varepsilon _i\)</span>
之间彼此不独立</li>
<li>模型可以拟合，但数据中有 outlier</li>
</ul>
<p>以下给出一些发现问题的方法：</p>
<h3 id="非线性关系">非线性关系</h3>
<p>简而言之，<span class="math inline">\(Y\)</span> 和 <span class="math inline">\(X\)</span>
之间可能并不是一个线性关系，但是我们采用了线性模型进行拟合。</p>
<p>找出问题的手段是使用 <span class="math inline">\(e_i \sim
X_i\)</span> 图诊断 <span class="math inline">\(Y\)</span> 和 <span class="math inline">\(X\)</span> 的线性关系是否过拟合。具体来说，给
<span class="math inline">\(e_i \sim X_i\)</span> 再做一个拟合图线
<code>scatter.smooth</code>，观察和 <span class="math inline">\(h=0\)</span> 是否偏离较大。虽然通过 <span class="math inline">\(Y \sim X\)</span> 也可以看出来，但是 <span class="math inline">\(e_i \sim X\)</span> 图更加明显。</p>
<p>即使线性关系是显著的（R 中得到 <span class="math inline">\(R^2\)</span> 较大、斜率 <span class="math inline">\(\beta_1\)</span>
显著），也不说明线性模型是最好的拟合模型。</p>
<h3 id="异方差问题">异方差问题</h3>
<ul>
<li><p>实际上的残差并不符合方差相等的假设，则称为异方差问题。即使发生这样的情况也未必会影响到
<span class="math inline">\(\beta_1,\beta_0\)</span>
的估计值，因为计算过程和这一假设实际上是无关的。但是，异方差问题会导致
<span class="math inline">\(b_0,b_1\)</span>
不再是使得方差最小的估计，失去了 BLUE
性质，但仍然是无偏的估计。问题会反映在关于 <span class="math inline">\(\beta_1,\beta_0\)</span>
的推断中，导致推断或者置信区间不是效率最高的。</p>
<p>举个例子，比如说 <span class="math inline">\(Y_i = 30 +100X_i + 10X_i
\varepsilon_i\)</span>，实际上是有 <span class="math inline">\(\varepsilon_i \sim N(0,10X_i
\sigma^2)\)</span>，方差并不相同。</p>
<p>画图检查最典型的异方差情况是画出 <span class="math inline">\(e_i \sim
X\)</span> 图后发现 <span class="math inline">\(X\)</span> 越大，<span class="math inline">\(e_i\)</span>
越分散，呈现出一个扇形的分布形态。</p></li>
<li><p>模型的诊断也可以使用一些理论方法，异方差检验中常用的几种检验如下所示：</p>
<ul>
<li><p>Bartlett 方法，本质上是 likelihood ratio
test，但非常依赖残差的正态假设。也即，如果检验结果是拒绝原假设，未必是真的发生了异方差现象，也可能是因为残差不服从正态分布造成了干扰。</p></li>
<li><p>Levene &amp; modified Levene (B.F.) 方法，非常常见。</p></li>
<li><p>对于可能影响方差导致异方差的因素 <span class="math inline">\(Z\)</span>，将 <span class="math inline">\(e_i
^2\)</span> 相对于 <span class="math inline">\(Z\)</span>
再做一次线性回归，得到的 SSR 记为 <span class="math inline">\(SSR^*\)</span>。</p>
<p>此时 <span class="math inline">\(LM = \frac{SSR^*/2}{(SSE/n)^2} \sim
\chi_1 ^2\)</span> 再进行检验。</p></li>
</ul></li>
</ul>
<h3 id="正态性假设">正态性假设</h3>
<p><span class="math inline">\(\varepsilon _i\)</span>
并不服从正态分布，这可以通过 QQ-plot 观察是否有 <span class="math inline">\(\varepsilon_{(i)} = \mu + \sigma
Z_i\)</span>，或者直接画 histogram 检查是否有正态的形状。</p>
<p><img src="https://s2.loli.net/2023/05/29/iQKNWpv8IseAoFj.png" alt="qq-plot.png"></p>
<p>理论方法之中 Shapiro-Wilk 方法是最佳的，有最大的
power，但是对于样本量是敏感的。也就是说，如果检验结果是 <span class="math inline">\(\varepsilon_i\)</span>
不服从正态分布，也可能是因为样本量太大导致了错误判断。</p>
<p>注意正态性检验是完全可以把 <span class="math inline">\(\{\varepsilon
_i \}\)</span> 作为一组数据放在一起观察整体的分布的，但是对 <span class="math inline">\(\{Y_i \}\)</span>
不可以这样做，它们彼此之间服从的是不同的条件分布。</p>
<h3 id="相关性">相关性</h3>
<p><span class="math inline">\(\varepsilon_i\)</span>
之间可能并不独立，有可能都受到 <span class="math inline">\(t_i\)</span>
的影响，等等。如果有类似可能考虑的因素可以对其做 sequence plot。</p>
<p>理论方法中最常用的是 Durbin-Watson 方法。</p>
<h3 id="outlier">Outlier</h3>
<p>模型的数据里有 outlier，不同性质的 outlier
对回归线的影响不尽相同，具体的在 Lecture 6
中再细说。简单来说就是，outlier 的 <span class="math inline">\(X_i\)</span> 越接近 <span class="math inline">\(\bar{X}\)</span>，对于回归线的斜率影响越小，但如果距离
<span class="math inline">\(\bar{X}\)</span>
比较远，则会产生比较强的杠杆效应。</p>
<p>即使模型中存在 outlier，参数的估计也可以是比较准确的。做 <span class="math inline">\(e_i \sim X\)</span> 图可以查看是否存在
outlier，也可以使用 <span class="math inline">\(Y_i \sim X_i\)</span>
的图来观察是否有距离回归线很远的点，这是最明显的一类
outlier。然后可以尝试剔除这样的点再重新做回归，检查各系数。</p>
<h2 id="关于-r2">关于 <span class="math inline">\(R^2\)</span></h2>
<p>如果在某一线性模型中得到 <span class="math inline">\(R^2 =
0.69\)</span>，能否说明这一线性模型是显著的？</p>
<ul>
<li>事实上可以。<span class="math inline">\(R^2 = 0.69\)</span>
已经是相对大的数值了，说明模型对于方差的解释能力是相对好的；另一方面考虑
<span class="math inline">\(r = \sqrt{R^2} &gt;
0.8\)</span>，相关系数其实是比较大的，可以认为这一模型是合适的。</li>
<li>但是，这并不能说明线性模型是这一问题下最好的模型。<span class="math inline">\(R^2\)</span>
想要多大就可以多大，例如给模型加入新的多项式型变量，总能更多地解释一些方差，不能单纯地追求
<span class="math inline">\(R^2\)</span> 的大小。</li>
<li>另外，如果在不同的模型比较中对 <span class="math inline">\(\{Y_i
\}\)</span> 做了变换，比如进行了标准化或者 Box-Cox
transform，此时是不能和原始的模型再进行 <span class="math inline">\(R^2\)</span> 的比较的，只能检查单个 <span class="math inline">\(R^2\)</span> 的值能否接受。这是因为 <span class="math inline">\(SST\)</span> 已经随着 <span class="math inline">\(\{Y_i \}\)</span>
的变化而变化了，模型的方差解释能力 <span class="math inline">\(\frac{SSR}{SST}\)</span> 无法比较。</li>
</ul>
<h1 id="lecture-6">Lecture 6</h1>
<h2 id="built-in-diagnostic-plots-in-r">Built-in Diagnostic Plots in
R</h2>
<p>R 内置的四个诊断图是 <span class="math inline">\(e_i \sim \hat
Y\)</span>，Q-Q plot，<span class="math inline">\(\sqrt{|\text{Standardized Residuals}|} \sim \hat
Y\)</span>，Cook's distance
图。前两个都比较简单，一个是万能的残差图，另一个是残差的正态性检验，主要说一下后两个图。</p>
<h3 id="scale-location-plot">Scale-Location Plot</h3>
<p><span class="math inline">\(\sqrt{|\text{Standardized Residuals}|}
\sim \hat Y\)</span> 作图体现了 residuals
随拟合值的分布情况，主要用来检验异方差。实际上在上一讲里面是直接用 <span class="math inline">\(e_i \sim X\)</span>
来检验异方差的，这里用了标准化的残差开方，有一些考虑：</p>
<ul>
<li><p>关注 magnitude 所以需要一个正值，选择了先取绝对值</p></li>
<li><p>取绝对值之后的数据往往人为造成了右偏，开方可以缓解一些</p></li>
<li><p>standardize 之后绝大多数 <span class="math inline">\(e_i\)</span>
都收入了 <span class="math inline">\([-2,2]\)</span>
这一范围内（正态分布的主要区间），更清晰，有可比性</p></li>
<li><p>虽然理论上的 <span class="math inline">\(\varepsilon_i\)</span>
是独立分布的，但是残差之间是有约束关系的，<span class="math inline">\(Var(e_i) = (1-h_{ii})\sigma^2\)</span>，也就是说
<span class="math inline">\(e_i\)</span> 的变动范围本身就和其在 <span class="math inline">\(x\)</span> 轴上的位置（也即 <span class="math inline">\(X_i\)</span> 的大小）相关。较大的 <span class="math inline">\(X_i\)</span> 会对应 <span class="math inline">\(e_i\)</span>
的更大方差，导致可能会看起来像异方差。</p>
<p>标准化并开方能够一定程度缓解这样的问题。</p></li>
</ul>
<p>如果此图上的残差点分布看起来比较随机，拟合线也相对平行于 <span class="math inline">\(x\)</span> 轴，基本可以认为是异方差。</p>
<h3 id="cooks-distance">Cook's Distance</h3>
<p>图中被标出数字的点/靠近右上角和右下角/红线之外的点需要重视一下，是
high leverage point 或者 outlier，可以进一步检验。</p>
<h2 id="residuals-leverage">Residuals &amp; Leverage</h2>
<h3 id="leverage">Leverage</h3>
<p>定义每一点的杠杆值为 <span class="math inline">\(h_{ii} =
\frac{\partial \hat Y_i}{\partial Y_i} \in [0,1]\)</span>，表征了某一个
<span class="math inline">\(X_i\)</span> 对应的 <span class="math inline">\(Y_i\)</span>
变动一个单位时，会导致回归线上的响应值变动的大小，也就是说，这一点的变化对于整个回归模型的影响。</p>
<p>事实上，<span class="math inline">\(X_i\)</span> 对应的点的 leverage
是 <span class="math inline">\(h_{ii} = \frac 1 n + \frac{(X_i - \bar
X)^2}{\Sigma_j (X_i - \bar X)^2}\)</span>，<span class="math inline">\(h_{ii}\)</span> 越大会导致 <span class="math inline">\(Var(e_i) = (1- h_{ii})\sigma^2\)</span> 越小，因此
<span class="math inline">\(e_i\)</span> 的变动范围变小，<span class="math inline">\(\hat Y_i\)</span>
能够变动的范围也较小，会导致回归线受较大的影响；从 leverage
的表达式可以看出来，<span class="math inline">\(|X_i -\bar X|\)</span>
越大会导致杠杆值越大，也就是说远离中心的点对回归线造成的影响较大。</p>
<p>我们对 extreme values 做一些分类：</p>
<ul>
<li><p>Outlier: 离群值是对于其 <span class="math inline">\(Y\)</span>
值而言的，<span class="math inline">\(Y_i\)</span>
的实际取值远离通常该有的范围，则这一点会被认为是 outlier。</p></li>
<li><p>High Leverage Point: 高杠杆值点是对于其 <span class="math inline">\(X_i\)</span> 的取值而言的，由上述分析可以得到
<span class="math inline">\(h_{ii}\)</span> 较大的主要条件。</p></li>
<li><p>Influential Point:
强影响力点指的是移除此点后，回归线会发生较大的变化的点。</p>
<p>如果一个点既是 outlier 又是 high leverage point，那它一定是
influential point，直觉上来看是因为它的 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span>
都具有一定的特征，会对回归线造成较大的影响。</p></li>
</ul>
<p>如果 <span class="math inline">\(h_{ii} &gt; 2p/n\)</span>
一般就称为高杠杆值点。</p>
<h3 id="studentized-residual">Studentized Residual</h3>
<p>一个 influential point
造成回归线的巨大改变无法在残差图上体现出来，因此我们希望有一种手段能够体现出它和其他正常点的巨大差异。一个自然的想法是先移除它再做回归线，然后在这一模型上体现残差，即为
studentized residual。</p>
<p>这么说还是太抽象了，放个图好了：</p>
<p><img src="https://s2.loli.net/2023/05/29/Uy8JvP4eBmWS1GT.png" alt="studentized-residual.png"></p>
<p>具体来说，studentized residual 和 standardized residual
有一些差别：</p>
<ul>
<li><p>standardized residual: <span class="math inline">\(\frac{e_i}{s(e_i)}\)</span>，由于 <span class="math inline">\(Var(e_i) = (1-h_{ii})\sigma^2\)</span>，则 <span class="math inline">\(s^2 (e_i) =
(1-h_{ii})MSE\)</span>，代入即可。</p></li>
<li><p>deleted residual: <span class="math inline">\(d_i = Y_i - \hat
Y_{i(-i)}\)</span>，其中 <span class="math inline">\(\hat
Y_{i(-i)}\)</span> 是 <span class="math inline">\(X_i\)</span>
在去除这一点的模型中所对应的响应。</p></li>
<li><p>studentized residual: <span class="math inline">\(\frac{d_i}{s(d_i)}\)</span>，同理有 <span class="math inline">\(s^2(d_i) = (1-h_{ii})MSE_i\)</span>，<span class="math inline">\(MSE_i\)</span> 是去除第 <span class="math inline">\(i\)</span> 点的模型对应的 MSE。</p>
<p>事实上 <span class="math inline">\(e_i^* = \frac{d_i}{s(d_i)} =
\frac{e_i}{\sqrt{MSE(1-h_{ii})}}\)</span> 是 internal studentized
residual，当 <span class="math inline">\(|e_i ^*| &gt;2\)</span>
时认为是一个 outlier。</p>
<p>studentized deleted residual 如下所示：</p>
<p><img src="https://s2.loli.net/2023/06/01/WTrmCsZ1oyLFzpQ.png" alt="studentized-deleted-residual.png"></p></li>
</ul>
<h3 id="cooks-distance-1">Cook's Distance</h3>
<p>考虑 <span class="math inline">\(D_i = \frac{\Sigma_j (\hat Y_j -
\hat Y_{j(-i)})^2}{ps^2}= \frac{e_i^2}{p\cdot MSE}
\frac{h_{ii}}{(1-h_{ii})^2}\)</span>，其中 <span class="math inline">\(\hat Y_{j(-i)}\)</span> 指的是 <span class="math inline">\(X_j\)</span> 在去除第 <span class="math inline">\(i\)</span> 点的模型中对应的平均响应值。第 <span class="math inline">\(i\)</span> 点的 Cook's distance <span class="math inline">\(D_i\)</span> 表征的是第 <span class="math inline">\(i\)</span>
点的值对全体预测值（也就是回归线）的影响力。<span class="math inline">\(D_i\)</span> 越大越说明这是个 high influential
point，一般吧 threshold 作为 <span class="math inline">\(0.5\)</span>
或者 <span class="math inline">\(\frac 4 n\)</span>。</p>
<h2 id="lack-of-fit-test">Lack of fit test</h2>
<p>怀疑某个模型并不符合线性，且其某一 <span class="math inline">\(X_i\)</span> 点处有多个对应的 <span class="math inline">\(Y\)</span>，也即这一点处存在 replicates 或者说
repeated observation 的时候，可以进行失拟检验。具体来说，可以进一步细分
SSE 为 sum of pure error 和 sum of lack of fit
error，前者由数据的随机性导致，后者由线性模型的失拟性造成。</p>
<p>事实上也是 general linear test 的一种，这里的 full model 就是 cell
mean model <span class="math inline">\(Y_{ij} = \mu_i +
\varepsilon_{ij}\)</span>，reduced model 是线性模型 <span class="math inline">\(Y_{ij}=\beta_0 +\beta_1 X_i +
\varepsilon_{ij}\)</span>。也就是说，实际上的 <span class="math inline">\(H_0: \mu_i = \beta_0 + \beta_1 X_i\)</span>，拒绝
<span class="math inline">\(H_0\)</span> 时说明 reduced model 失拟。</p>
<p>对 SSE 进行进一步的拆分，将其改变为 <span class="math inline">\(SSE =
SSPE +SSLF\)</span>，具体表达式和自由度如下所示：</p>
<p><span class="math display">\[\Sigma_{i=1} ^c
\Sigma_{j=1}^{n_i}(Y_{ij} - \hat Y_{ij})^2 = \Sigma_{i=1} ^c
\Sigma_{j=1}^{n_i}(Y_{ij} - \bar Y_{i.})^2+ \Sigma_{i=1} ^c
\Sigma_{j=1}^{n_i}(\bar Y_{i.} - \hat Y_{ij})^2\]</span></p>
<p><span class="math display">\[SSE = SSPE + SSLF\]</span></p>
<p><span class="math display">\[(n-2) = (n-c)+(c-2)\]</span></p>
<p>因此，对应地有 <span class="math inline">\(E(MSPE) = \sigma^2,E(MSLF)
= \sigma^2 + \frac{\Sigma_i n_i (\mu_i - (\beta_0 + \beta_1
X_i))^2}{c-2}\)</span>，test statistic 是 <span class="math inline">\(F^* = \frac{MSLF}{MSPE} \sim ^{H_0}
F(n-c,c-2)\)</span>。</p>
<p>这也就说明了为什么失拟检验只有在存在 replicates
的时候才能做，因为这时才会有 <span class="math inline">\(n &gt;
c\)</span>，使得模型不至于退化。</p>
<h2 id="remedy-methods">Remedy Methods</h2>
<h3 id="补救非线性">补救非线性</h3>
<p>通过 <span class="math inline">\(e_i \sim X\)</span>
观察到模型并不是完全线性的时候，如果不想再改变
assumptions，可以转而选择非线性的模型。</p>
<p>R 中可以调用函数 <code>nls</code>。</p>
<h3 id="补救异方差">补救异方差</h3>
<p>可以使用 weighted analysis，具体参见 Lecture 12 的内容。</p>
<h3 id="补救非正态">补救非正态</h3>
<p>如果残差体现出非正态分布的性质，可以对 <span class="math inline">\(Y\)</span>
做变换继续使用线性模型，也可以使用其他的模型来允许残差不同分布。在 R
中可以调用函数 <code>glm</code>。</p>
<h3 id="transformation">Transformation</h3>
<p>有以下任一需求的时候都可以考虑对 <span class="math inline">\(Y_i\)</span> 做变换：</p>
<ul>
<li>稳定方差：观察到 <span class="math inline">\(\varepsilon_i\)</span>
似乎是异方差的</li>
<li>提高正态性</li>
<li>简化模型，提高解释性</li>
</ul>
<p>最普遍的方法是 Box-Cox Transformation，取 <span class="math inline">\(Y^* =
(Y^\lambda-1)/\lambda\)</span>，首要任务是找到最合适的 <span class="math inline">\(\lambda\)</span>
做变换。事实上所谓的“最合适”包含两层意义，其一是希望模型拟合程度较高，其二是希望
<span class="math inline">\(\lambda\)</span> 使得 <span class="math inline">\(Y^*\)</span>
的形式尽量简洁明了，解释性强，也就是取所谓的 convenient lambda。例如取
<span class="math inline">\(\lambda=0\)</span> 时有 <span class="math inline">\(Y^* = \log Y\)</span>，取 <span class="math inline">\(\lambda\)</span>
为整数时即为多项式回归，等等。</p>
<p>另外不同的 <span class="math inline">\(\lambda\)</span>
值会对数据的右偏有不同程度的改善，右尾压缩最明显的是开根和取对数；也可以取负值来压缩左尾。</p>
<p>在实际使用中往往是直接对 <span class="math inline">\(\lambda\)</span>
取一个 sequence，找到近似取到 maximum likelihood 或者使得 SSE 最小的
<span class="math inline">\(\lambda\)</span>
的一个置信区间，再在其中寻找合适的 convenient lambda。</p>
<p>一些典型的数据分布和变换方法：</p>
<p><img src="https://s2.loli.net/2023/05/29/VwIlizq5vsEFot6.png" alt="prototype-box-cox-trans.png"></p>
<h2 id="miscellaneous-topics">Miscellaneous Topics</h2>
<p>一些杂谈，关于 simple linear regression 的最后内容。</p>
<h3 id="regression-through-the-origin">Regression Through the
Origin</h3>
<p>非常坏回归，爱来自自由度（</p>
<p>强迫过原点回归的时候斜率的估计是 <span class="math inline">\(\hat
\beta_1 = \frac{\Sigma X_iY_i}{\Sigma
X_i^2}\)</span>，这会导致一些很严重的后果：</p>
<ul>
<li>残差的和 <span class="math inline">\(\Sigma e_i \neq
0\)</span>，这导致 <span class="math inline">\((Y_i - \bar Y)^2 = (Y_i -
\hat Y_i)^2 + (\hat Y_i - \bar Y)^2 + 2(Y_i - \hat Y_i)(\hat Y_i - \bar
Y)\)</span> 的交互项无法消去，于是也不能再对 SST 做分解成为 SSE 和 SSR
之和。</li>
<li>SSE 此时的自由度是 <span class="math inline">\(n-1\)</span>，SST
的自由度也不再是 SSE 和 SSR 的自由度之和。</li>
</ul>
<h3 id="inverse-predictions">Inverse Predictions</h3>
<p>对 <span class="math inline">\(Y \sim X\)</span> 做回归得到 <span class="math inline">\(y = b_0 + b_1 x\)</span> 和对 <span class="math inline">\(X \sim Y\)</span> 做回归得到 <span class="math inline">\(x = a_0 + a_1 y\)</span>
中，实际上在绝大多数情况下都有 <span class="math inline">\(a_1 b_1 \neq
1\)</span>。也就是说，两种回归的结果不能简单地用线性求反函数得到，这听起来有点反直觉但确实是合理的，做
<span class="math inline">\(Y \sim X\)</span> 回归的目标是取得关于 <span class="math inline">\(Y\)</span> 的最小残差平方和，做 <span class="math inline">\(X\sim Y\)</span> 回归的目标是取得关于 <span class="math inline">\(X\)</span>
的最小残差平方和，目标不一致得到的结果自然不同。</p>
<p>用理论来解释的话，取相关系数 <span class="math inline">\(r =
\frac{\sum (X_i - \bar X)(Y_i - \bar Y)}{\sqrt{\sum (X_i - \bar X)^2}
\sqrt{\sum (Y_i - \bar Y)^2}}\)</span>，记 <span class="math inline">\(S_Y = \sqrt{\sum (Y_i - \bar Y)^2},S_X =
\sqrt{\sum (X_i - \bar X)^2}\)</span>，于是有：</p>
<p><span class="math inline">\(b_1 = r \frac{S_Y}{S_X},a_1 = r
\frac{S_X}{S_Y}\)</span>，因此 <span class="math inline">\(b_1a_1 =
r^2\)</span>。两条回归线之间的夹角是 <span class="math inline">\(\tan
\theta = \frac{1-r^2}{r(\frac{S_X}{S_Y} +
\frac{S_Y}{S_X})}\)</span>。</p>
<p>如果对于某个响应值 <span class="math inline">\(Y_i\)</span>
需要反向预测输入变量 <span class="math inline">\(X_i\)</span>，应该选择
<span class="math inline">\(\hat X_h = \frac{Y_h - b_0}{b_1}\)</span>
而不是反过来做 <span class="math inline">\(X\)</span> 关于 <span class="math inline">\(Y\)</span> 的回归。</p>
<h3 id="limitations-of-r2">Limitations of <span class="math inline">\(R^2\)</span></h3>
<p>使用 <span class="math inline">\(R^2\)</span>
作为判断依据的时候要注意以下问题：</p>
<ul>
<li><p><span class="math inline">\(R^2\)</span>
不能作为拟合程度的度量。<span class="math inline">\(R^2 = \frac{b_1 ^2
\sum (X_i - \bar X)^2}{\sum (Y_i - \bar Y)^2}\)</span>
的形式显示了，实际上如果把 <span class="math inline">\(X\)</span>
的取值变得足够分散，<span class="math inline">\(R^2\)</span>
的取值想要多大就能有多大。</p>
<p>不同的散点分布情况可以得到几乎相同的 <span class="math inline">\(R^2\)</span>，一定要画图检查线性模型是不是最合理的。</p>
<p><img src="https://s2.loli.net/2023/05/29/OJZi635KwUQ8quf.png" alt="same-r-square-but-different-models.png"></p></li>
<li><p>不能对于不同的模型比较 <span class="math inline">\(R^2\)</span>，归根结底 <span class="math inline">\(R^2\)</span> 反映的是模型对于 SST 的解释能力，SST
一旦改变就不能交叉对比。因此上述 transformation 中对于系数 <span class="math inline">\(\lambda\)</span> 的比选标准是 maximum
likelihood。</p></li>
<li><p><span class="math inline">\(R^2\)</span> 不能显示 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 之间的因果关系，因为 <span class="math inline">\(Y\sim X\)</span> 和 <span class="math inline">\(X
\sim Y\)</span> 这两种回归得到的 <span class="math inline">\(R^2\)</span> 是相等的。</p></li>
</ul>
<h1 id="lecture-7">Lecture 7</h1>
<p>从一元线性回归过渡到多元回归的部分，介绍回归方程的矩阵表达。</p>
<h2 id="矩阵表达">矩阵表达</h2>
<p>把 <span class="math inline">\(n\)</span> 个回归方程的形式改成：</p>
<p><span class="math display">\[\begin{bmatrix}Y_1 \\ Y_2 \\ ... \\ Y_n
\end{bmatrix} = \begin{bmatrix}1 &amp; X_{1,1} &amp; X_{2,1} &amp;...
&amp; X_{p-1,1} \\ 1 &amp; X_{1,2} &amp; X_{2,2} &amp; ... &amp;
X_{p-1,2} \\ ... &amp; ... &amp; ... &amp;... &amp; ... \\ 1 &amp;
X_{1,n} &amp; X_{2,n} &amp; ... &amp; X_{p-1,n} \end{bmatrix}
\begin{bmatrix}\beta_0 \\ \beta_1 \\ ... \\ \beta_{p-1} \end{bmatrix} +
\begin{bmatrix}\varepsilon_1 \\ \varepsilon_2  \\ ... \\ \varepsilon_n
\end{bmatrix}\]</span></p>
<p><span class="math display">\[Y = X \beta + \varepsilon\]</span></p>
<p>假设 <span class="math inline">\(\varepsilon \sim N_n(0, \sigma^2
I_{n \times n})\)</span>，则有 <span class="math inline">\(Y \sim N_n(X
\beta , \sigma^2 I_{n \times n})\)</span>，以下考虑参数矩阵 <span class="math inline">\(\beta\)</span> 的估计：</p>
<p><span class="math display">\[b = \hat \beta = (X^TX)^{-1} X^T Y \sim
N_p(\beta , (X^TX)^{-1} \sigma^2)\]</span></p>
<p>和 simple linear regression 相同，<span class="math inline">\(b\)</span> 也有 BLUE 的性质。</p>
<p>因此有 <span class="math inline">\(Y\)</span> 的平均响应为 <span class="math inline">\(\hat Y = X \hat \beta = X(X^TX)^{-1}
X^TY\)</span>，其中记 <span class="math inline">\(H = X(X^TX)^{-1}
X^T\)</span>，这就是著名的 hat
matrix，围绕它有很多性质，后面再说；由于残差 <span class="math inline">\(e = Y - \hat Y = (I - X(X^TX)^{-1}
X^T)Y\)</span>，因此 simple linear regression 中对于 <span class="math inline">\(e\)</span> 的限制在此处相应地修改为 <span class="math inline">\(e^T \hat Y = Y^T(I - X(X^TX)^{-1} X^T)
(X(X^TX)^{-1} X^T)Y = 0\)</span>。</p>
<p>相应地，<span class="math inline">\(\sigma\)</span> 的估计量可以改成
<span class="math inline">\(s^2 = \frac{e^Te}{n-p} =
\frac{Y^T(I-H)Y}{n-p}\)</span>，这就是 MSE，是 <span class="math inline">\(\sigma^2\)</span> 的无偏估计；又因为 <span class="math inline">\(Var(b) = (X^TX)^{-1} \sigma^2\)</span>，因此有
<span class="math inline">\(\hat Var(b) = (X^TX)^{-1} \hat \sigma^2 =
s^2 (X^TX)^{-1}\)</span> 是 <span class="math inline">\(Var(b)\)</span>
的估计。</p>
<p>ANOVA 中的方差和自由度拆分在这里仍然适用：</p>
<p><span class="math display">\[SST = (Y - \bar Y \mathbb 1_n)^T (Y -
\bar Y \mathbb 1_n), \quad df = n-1\]</span></p>
<p><span class="math display">\[SSM = (\hat Y - \bar Y \mathbb
1_n)^T(\hat Y - \bar Y \mathbb 1_n), \quad df = p-1\]</span></p>
<p><span class="math display">\[SSE  = (Y - \hat Y)^T(Y- \hat Y), \quad
df=n-p\]</span></p>
<p><span class="math inline">\(F-\)</span>test 的检验统计量是 <span class="math inline">\(F^* = \frac{MSM}{MSE} \sim^{H_0}
F_{p-1,n-p}\)</span>，其中假设是 <span class="math inline">\(H_0
:\beta_1 = \beta_2 = ... =\beta_{p-1}=0\)</span>；决定系数 <span class="math inline">\(R^2 = \frac{SSM}{SST}\)</span>
仍然保持不变，显示了线性模型解释方差的能力；adjusted <span class="math inline">\(R^2\)</span> 定义为 <span class="math inline">\(1-
\frac{MSE}{MST} = 1- \frac{n-1}{n-p} \frac{SSE}{SST}\)</span>，相对于
<span class="math inline">\(R^2\)</span> 的优势在于，<span class="math inline">\(R^2\)</span>
在有任意的变量进入模型时都会增大，但如果新变量的显著性不足会导致
adjusted <span class="math inline">\(R^2\)</span>
下降，是一个更有力的参数。</p>
<h2 id="hat-matrix">Hat Matrix</h2>
<p>Hat Matrix 有丰富的性质，在这里列举一些和统计关联比较大的。</p>
<p><span class="math display">\[Cov(e) = Cov((I-H)Y) = (I-H)^T \sigma^2
(I-H) = \sigma^2 (I-H)\]</span></p>
<p><span class="math display">\[Cov(e_i , e_j) = -\sigma^2
h_{ij}\]</span></p>
<p><span class="math display">\[Var(e_i) = \sigma^2
(1-h_{ii})\]</span></p>
<p>其中 <span class="math inline">\(h_{ij}\)</span> 是矩阵 <span class="math inline">\(I -H\)</span> 的分量。</p>
<p>事实上，第 <span class="math inline">\(i\)</span> 点的杠杆值就是
<span class="math inline">\(h_{ii}\)</span>。可以通过 <span class="math inline">\(H = H^2\)</span> 这一性质简单地证明出 <span class="math inline">\(h_{ii} \leq
1\)</span>，这也和杠杆值的定义是符合的。</p>
<h2 id="multiple-linear-regression">Multiple Linear Regression</h2>
<ul>
<li><p>关于多元回归的系数 <span class="math inline">\(\beta_1,\beta_2,...,\beta_{p-1}\)</span>（也称为偏回归系数），我们仍然可以按照
SLR 时的方式解释它们：<span class="math inline">\(\beta_i\)</span> 是在
<span class="math inline">\(X_1,X_2,...,X_{i-1},X_{i+1},...,X_{p-1}\)</span>
不变时，<span class="math inline">\(X_i\)</span> 变化一个单位导致 <span class="math inline">\(E(Y)\)</span> 的变化量。</p>
<p>实际上这就引出了多元回归的一个巨大隐患：并不是所有的变量都完全不相关，一旦
<span class="math inline">\(X_1,X_2\)</span> 之间有相关性存在，改变
<span class="math inline">\(X_1\)</span> 的时候很难保证 <span class="math inline">\(X_2\)</span>
不变，多元回归的系数解释性因此变差。</p>
<p>虽然理论上确实可能存在完全不相关的变量，但是对应的数据也很难不相关。</p></li>
<li><p><span class="math inline">\(Var(e_i) = \sigma^2
(1-h_{ii})\)</span>，因此在线性回归中，位于中间（靠近 <span class="math inline">\(\bar X\)</span>）的 <span class="math inline">\(X\)</span>
拟合能力较弱（杠杆值低，对回归线的影响较弱）但是预测能力较好（<span class="math inline">\(Var(e_i)\)</span> 较小，<span class="math inline">\(\hat Y\)</span>
能够变动的范围小）。相反地，位于两端的 <span class="math inline">\(X\)</span> 拟合能力较强但是预测能力较弱。</p>
<p>事实上我们是不能轻易预测已有数据范围之外的 <span class="math inline">\(x\)</span>
的响应的，理由如上所述，此时的预测能力很弱。</p></li>
</ul>
<h1 id="lecture-8">Lecture 8</h1>
<h2 id="explanatory-data-analysistransformation">Explanatory Data
Analysis——Transformation</h2>
<h3 id="why-look-at-y">Why look at Y</h3>
<p>一般来说 <span class="math inline">\(Y\)</span>
的分布我们是不用太在意的，毕竟不是同分布，但其实考虑到做变换的话还是要稍微看一下它的分布。</p>
<p>如果分布是高度有偏的，做变换把长尾的部分往中央收一收可以得到的效果有：</p>
<ul>
<li><p>减小 SST，修正模型的显著性；</p></li>
<li><p>把拖尾方向可能的 outlier
向内收，有可能可以变成正常的数据来使用；另一侧原来数据比较集中，做变换如果可以将分布拉长的话便于观察其中的一些特征；</p>
<p>实际上 outlier
并不能随意的扔掉，做变换的想法是能够保留就尽量保留。有些时候很多现象就隐藏在出现了
outlier
这件事情上面，比如臭氧层空洞没有被尽早发现就是因为相关的数据被当成
outlier 扔掉了。</p></li>
<li><p>方差对于 skewed data 和 outlier 都比较敏感；</p></li>
<li><p>对于 skewed data，均值并不是中心位置很好的显示。</p></li>
</ul>
<p>但通常来说做变换之前都需要三思：</p>
<ul>
<li>做变换后可解释性会有问题，比如 Box-Cox Transformation 中奇怪的 <span class="math inline">\(\lambda\)</span>
取值会导致可解释性变差，实验数据有的时候需要保留单位，做变换之后会失去意义；</li>
<li>会导致 <span class="math inline">\(H_0\)</span> 改变；</li>
<li>不一定能够改进正态性，做变换未必有好的效果；</li>
<li>做了变换得到结果之后，变回原始数据很可能破坏无偏性；</li>
<li>破坏了残差的分布。</li>
</ul>
<p>有一些平替方案：GLM, resampling methods, non-parametric methods</p>
<p>另外我们一般不会对 <span class="math inline">\(X\)</span>
做变换，除非做变换之后和 <span class="math inline">\(Y\)</span>
有非常明确的关系，另一个原因是做变换可能会导致共线性。</p>
<h3 id="why-log-transformation">Why log Transformation</h3>
<p>对 right-skewed data 做 log transformation
的好处是显著多于其他类型的变换的：</p>
<ul>
<li><span class="math inline">\(\log Y_1 - \log Y_2 = \log (1+ \frac{Y_1
- Y_2}{Y_2}) \approx \frac{Y_1 -
Y_2}{Y_2}\)</span>，可以把绝对误差变为相对误差来讨论；</li>
<li><span class="math inline">\(\log Y\)</span>
可以让数据的分布更对称，出于计算的考虑</li>
<li>如果 <span class="math inline">\(Y\)</span> 全部都是正数，但 <span class="math inline">\(Y = X \beta + \varepsilon \sim N(X \beta, \sigma^2
I)\)</span>，一个多元正态分布的数据全部是正数的概率非常小，假设不合理。</li>
</ul>
<p>但是最大的问题就是可解释性。对于参数 <span class="math inline">\(\beta\)</span> 的解释是在 <span class="math inline">\(X\)</span> 增长一个单位时 <span class="math inline">\(Y\)</span>
的平均响应的对应变化，在这里我们选择的是报告平均响应变化的百分比，也即
<span class="math inline">\(\frac{e^{(X+1)b} - e^{Xb}}{e^{Xb}} = e^b
\times 100 \%\)</span>，作为一个补救措施。</p>
<h2 id="mlr-vs-slr">MLR vs SLR</h2>
<p>相比于 simple linear regression，MLR
的变量增多了之后需要考虑的问题也增加了，复杂度也变大了。</p>
<h3 id="变量选择">变量选择</h3>
<p>破事很多：</p>
<ul>
<li>单独一个 <span class="math inline">\(X_i\)</span>
在模型里不显著也不能直接扔掉，它可能是 suppressor
variable，会让别的变量显著</li>
<li>多个变量的模型显著不能推出单个变量显著</li>
<li>单变量模型中，变量显著性 <span class="math inline">\(t\)</span>
检验和模型显著性 <span class="math inline">\(F\)</span> 检验的 p-value
相等，因为 <span class="math inline">\(F_{1,n} = t_n ^2\)</span></li>
<li>其余变量是否需要进入模型，可以先考虑残差对于其余各个变量的回归显著性，先测试最显著的变量进入模型</li>
</ul>
<p>有的时候我们认为模型里变量越多越好，有的时候越少越好，这取决于做回归的目的：</p>
<ul>
<li>回归模型是为了预测：变量越多或者说 adjusted <span class="math inline">\(R^2\)</span>
越大，解释的方差越多，预测水平越好</li>
<li>回归模型是为了解释：变量越少，explanatory variable 和 response
之间的关系越明确，解释性越好</li>
</ul>
<p>总之，会随着模型中进入的变量而改变的参数有：</p>
<ul>
<li>回归系数（<span class="math inline">\(\hat \beta_i\)</span>）</li>
<li>standard error</li>
<li>模型显著性</li>
</ul>
<h3 id="诡异的现象">诡异的现象</h3>
<p>有的时候会遇到 Significance &amp; low <span class="math inline">\(R^2\)</span>
同时出现的情况，也就是某个变量看起来是显著的，但是 <span class="math inline">\(R^2\)</span>
很低，解释方差的能力并不好。这是正常的现象。</p>
<ul>
<li><p>回到 <span class="math inline">\(R^2\)</span> 的定义和 F
检验的本质可以发现，<span class="math inline">\(R^2 = \frac{SSR}{SST},
F^* = \frac{MSR}{MSE} = \frac{R^2}{1-R^2} \frac{n-p}{p-1}\)</span>，如果
<span class="math inline">\(n\)</span> 非常大，即使 <span class="math inline">\(R^2\)</span> 很小也和 <span class="math inline">\(F^*\)</span> 很大之间并不矛盾。</p></li>
<li><p><span class="math inline">\(R^2\)</span> 的分母 <span class="math inline">\(SST\)</span>
实际上表征了数据的分散程度，数据非常分散的时候是可能导致 <span class="math inline">\(R^2\)</span>
减小的。但是数据分散和存在线性并不矛盾，图中的两个线性关系当然都显著，但是
<span class="math inline">\(R^2\)</span> 有巨大的差距。</p>
<p><img src="https://s2.loli.net/2023/05/29/BkYA4IrhbLXjTna.png" alt="central-tendency-and-variability.png"></p></li>
</ul>
<h3 id="inference">Inference</h3>
<ul>
<li><p>多重线性回归的推断里面也有一个著名定理：<span class="math inline">\(Y \sim N_n(X \beta , \sigma^2 I)\)</span>，于是有
<span class="math inline">\(b \sim N_p(\beta , \sigma^2
(X^TX)^{-1})\)</span>，以及：</p>
<ul>
<li><span class="math inline">\(\frac{e^Te}{\sigma^2} =
\frac{SSE}{\sigma^2} \sim \chi^2_{n-p}\)</span></li>
<li><span class="math inline">\(b\)</span> 和 <span class="math inline">\(SSE\)</span> 相互独立</li>
</ul>
<p>由此得到很多推断方法。</p></li>
<li><p>关于 <span class="math inline">\(b_k\)</span> 的推断和
CI，主要关注 <span class="math inline">\(b_k \sim N(\beta_k,
\sigma^2((X^TX)^{-1})_{k,k})\)</span>，希望检验 <span class="math inline">\(H_0 : \beta_k =0; H_1 : \beta_k \neq
0\)</span>。</p>
<p>检验统计量是 <span class="math inline">\(t^* = \frac{b_k}{s(b_k)} =
\frac{b_k}{\sqrt{MSE ((X^TX)^{-1})_{k,k}}}\sim ^{H_0}
t_{n-p}\)</span>，由此可以检查单个变量的显著性。注意此处所谓的显著性，指的是第
<span class="math inline">\(k\)</span>
个变量最后一个进入模型时的显著性，无论它在 R table 里排列在哪里。和
general linear test 的结果一致。如果这一检验体现出来变量 <span class="math inline">\(X_k\)</span>
不显著，绝大多数情况下是可以不保留的。</p>
<p><span class="math inline">\(100(1-\alpha) \%\)</span> 置信区间是
<span class="math inline">\((b_k \pm t_{1-\alpha /2,n-p}
s(b_k))\)</span>，注意 <span class="math inline">\(s(b_k)\)</span>
的含义在上述已经提到。</p></li>
<li><p>关于 <span class="math inline">\(E(Y_h)\)</span> 的推断和
CI，考虑 <span class="math inline">\(\mu_h = E(Y_h) = X_{h}
\beta\)</span>。</p>
<p>其估计量是 <span class="math inline">\(\hat \mu_h = X_h b = X_h
(X^TX)^{-1} X^TY \sim N(\mu_h , \sigma^2 X_h (X^TX)^{-1}
X_h^T)\)</span>，于是 <span class="math inline">\(s^2(\hat \mu_h) = MSE
\cdot X_h (X^TX)^{-1} X_h^T\)</span>。因此 <span class="math inline">\(\mu_h\)</span> 的 <span class="math inline">\(100
(1-\alpha) \%\)</span> 置信区间是 <span class="math inline">\((\hat
\mu_h \pm t_{1-\alpha /2 , n-p} s(\hat \mu_h))\)</span></p>
<p>考虑 <span class="math inline">\(Y_h = X_h \beta +
\varepsilon\)</span>，有 <span class="math inline">\(\hat Y_h - Y_h \sim
N(0,\sigma^2 + \sigma^2 X_h (X^TX)^{-1} X_h^T)\)</span>， <span class="math inline">\(s^2(\hat Y_h - Y_h) = MSE \cdot (1+X_h (X^TX)^{-1}
X_h ^T)\)</span>，因此 <span class="math inline">\(Y_h\)</span> 的 <span class="math inline">\(100 (1-\alpha) \%\)</span> 置信区间是 <span class="math inline">\((\hat Y_h \pm t_{\alpha /2 , n-p} s(\hat Y_h -
Y_h))\)</span></p></li>
</ul>
<h1 id="lecture-9">Lecture 9</h1>
<h2 id="extra-sum-of-squares">Extra Sum of Squares</h2>
<p>想法很简单，定义就是把一个新的变量加入模型后可以额外解释的方差，例如模型中本来存在
<span class="math inline">\(X_2\)</span> 时，加入 <span class="math inline">\(X_1\)</span> 后可以额外解释的方差是 <span class="math inline">\(SSR(X_1 | X_2) = SSR(X_1 ,X_2) - SSR(X_2) =
SSE(X_2) - SSE(X_1,X_2)\)</span>。</p>
<p>在多重回归中分解 sum of squares 的方式最常见的是 Type I method：</p>
<p><span class="math inline">\(SSR(X_1,X_2,X_3,X_4) = SSR(X_1) + SSR(X_2
|X_1) + SSR(X_3 |X_1,X_2)+SSR(X_4 |X_1,X_2,X_3)\)</span></p>
<p>也就是所谓的 sequential sum of squares 的方法，在
<code>anova()</code> 中的列表就是这样的分解方式，分掉了所有的 SSR。</p>
<p>SAS 中的 sum of square 分解有三种模式，以考虑 A,B,AB 三种因子（2-way
ANOVA）的情况如下排列：</p>
<p><img src="https://s2.loli.net/2023/05/30/KrAcZO2aIPQvYlS.png" alt="3-types-anova.png"></p>
<p>可以看到 type I 就是按照 sequential
的模式进入模型，认为不同的变量有重要性的排序，先进入理论上来说最有必要进入模型的
A，再进入 B，最后进入 AB，分别计算 extra sum of squares；type II
忽略了交互效应 AB，对于 A 和 B 的单变量分解是与 type I 相同的；type III
和之前提到的 t 检验类似，每个模型的 extra sum of square
分解都是考虑它最后一个进入模型时带来的方差解释能力，但这里实际上存在一个问题，按照
hierarchy 的原则来说如果主效应 A,B
之一不显著/没有进入模型，是不能允许交互项 AB 进入模型的。</p>
<h2 id="general-linear-test-1">General Linear Test</h2>
<h3 id="test-reduced-model">Test Reduced Model</h3>
<p>一种检验 full model 和 reduced model
之间关系的检验方法，例如对于存在五个变量 <span class="math inline">\(X_1,X_2,X_3,X_4,X_5\)</span> 的 full model
和存在三个变量 <span class="math inline">\(X_1,X_2,X_3\)</span> 的
reduced model 来说，假设 <span class="math inline">\(H_0: \beta_4 =
\beta_5 = 0; H_1:\beta_4,\beta_5\)</span> 中至少有一个不是 <span class="math inline">\(0\)</span>。</p>
<p>检验统计量是 <span class="math inline">\(F^* = \frac{(SSE(R) -
SSE(F))/(df_E(R) - df_E(F)}{SSE(F)/df_E(F)}\sim ^{H_0}
F_{df_E(R)-df_E(F),df_E(F)}\)</span>，注意第一个自由度实际上是两个模型相差的变量个数，分母中的
<span class="math inline">\(SSE(R) - SSE(F) = SSR(F) - SSE(R) =
SSR(X_4,X_5|X_1,X_2,X_3)\)</span> 实际上是 extra sum of squares。</p>
<p>事实上我们是 prefer 接受 <span class="math inline">\(H_0\)</span>
的，就相当于可以使用变量更少的模型，解释性更强；但是有的时候检验做出来接受
<span class="math inline">\(H_0\)</span>，也需要考虑一下是不是数据量太小导致
power 不够，如果数据量够大就可以放心地使用 reduced model 了。</p>
<h3 id="test-linear-hypothesis">Test Linear Hypothesis</h3>
<p>实际上只要是关于回归系数的线性检验就都可以用 general linear test
来进行，比如 <span class="math inline">\(H_0 : \beta_1 + 3 \beta_2
=12\)</span>，等等。</p>
<p><span class="math inline">\(H_0 : C \beta =t\)</span>，<span class="math inline">\(C\)</span> 是有关 <span class="math inline">\(\beta\)</span> 的约束条件的矩阵，检验统计量是
<span class="math inline">\(F = \frac{(C \hat \beta - t)^T (C(X^TX)^{-1}
C^T)^{-1} (C \hat \beta - t)}{qs^2} \sim^{H_0} F_{q,n-p}\)</span>，其中
<span class="math inline">\(q=rank(C)\)</span>。</p>
<h2 id="偏决定系数-偏相关系数">偏决定系数 &amp; 偏相关系数</h2>
<p><span class="math inline">\(R^2_{Yk|1,2,...,k-1,k+1,...,q} =
\frac{SSR(X_k |
X_1,..,X_{k-1},X_{k+1},...,X_q)}{SSE(X_1,...,X_{k-1},X_{k+1},...,X_q)} =
1-
\frac{SSE(X_1,X_2,...,X_q)}{SSE(X_1,...,X_{k-1},X_{k+1},...,X_q)}\)</span></p>
<p>本质上说的是，模型中新进入的 <span class="math inline">\(X_k\)</span>
带来的 extra sum of squares 解释了 SSE
的比例，也即解释了没有由原来的变量解释掉的方差比率。</p>
<p>实际上另一种表现形式可以是，我们认为 <span class="math inline">\(R^2_{Yk|1,2,...,k-1,k+1,...,q} =
\frac{SSE(R)-SSE(F)}{SSE(R)} = \frac{SSR(X_k |X_{-k})}{SST(Y|X_{-k})} =
R^2(Y|X_{-k}, X_k|X_{-k})\)</span></p>
<p>也就是说，实际上是对于 <span class="math inline">\(Y|X_{-k} \sim
X_k|X_{-k}\)</span> 这两组残差做线性回归，得到的决定系数正好是 <span class="math inline">\(X_k\)</span> 的偏决定系数。</p>
<p>实际上偏相关系数 <span class="math inline">\(r_{k|1,2,...,k-1,k+1,...,q} = sign(\hat
\beta_k)\sqrt{R^2_{Yk|1,2,...,k-1,k+1,...,q}}\)</span>
表征的就是这两组残差之间的相关系数，如果有 <span class="math inline">\(0
&lt; r_{3|12} &lt; r_{12}\)</span> 就说明变量 <span class="math inline">\(X_3\)</span> 可以部分解释 <span class="math inline">\(X_1,X_2\)</span> 之间的相关性。</p>
<h2 id="标准回归">标准回归</h2>
<h3 id="motivation">Motivation</h3>
<ul>
<li>如果 <span class="math inline">\(X_1,X_2\)</span>
之间的尺度差距过大会导致 <span class="math inline">\(\beta_1,\beta_2\)</span>
的尺度也有差距，无法直接比较，也可能会影响变量显著性。</li>
<li>会导致 designed matrix 接近不满秩，计算逆矩阵出现问题。</li>
</ul>
<h3 id="methodcorrelation-transformation">Method——Correlation
Transformation</h3>
<p>考虑 <span class="math inline">\(s_Y = \sqrt{\frac{\sum_i (Y_i - \bar
Y)^2}{n-1}}, s_{X_k} = \sqrt{\frac{\sum_i (X_{ik}-\bar
X_k)^2}{n-1}}\)</span>，对变量和响应值分别做标准化：</p>
<p><span class="math display">\[\begin{aligned} \frac{1}{\sqrt{n-1}}
\frac{Y_i - \bar Y}{s_Y}&amp; = \frac{1}{\sqrt{n-1}} \frac{(\beta_0 +
\beta_1 X_{i1} + \beta_2 X_{i2} + \varepsilon_i)-(\beta_0 + \beta_1 \bar
X_1 + \beta_2 \bar X_2 + \bar
\varepsilon)}{s_Y}\\&amp;=\frac{1}{\sqrt{n-1}}\frac{\beta_1(X_{i1} -
\bar X_1) + \beta_2 (X_{i2}-\bar X_2) + (\varepsilon_i - \bar
\varepsilon)}{s_Y}\\&amp;=\frac{\beta_1 s_{X1}}{s_Y} \frac{(X_{i1}-\bar
X_1)}{\sqrt{n-1} s_{X1}} + \frac{\beta_2 s_{X2}}{s_Y} \frac{(X_{i2}-\bar
X_2)}{\sqrt{n-1} s_{X2}} + \frac{\varepsilon_i - \bar
\varepsilon}{\sqrt{n-1}s_Y} \end{aligned}\]</span></p>
<p><span class="math display">\[Y_i^* = 0 + \beta_1^* X_{i1}^* +
\beta_2^* X_{i2}^* + ... +\beta_{p-1}^* X_{i,p-1}^* +
\varepsilon_i\]</span></p>
<p>其中 <span class="math inline">\(\beta_k ^* = \frac{s_{X_k}}{S_Y}
\beta_k\)</span> 是新的回归系数。</p>
<ul>
<li><p>注意这个方法是有一些问题的，比如说破坏了残差的假设，以及强迫过原点。</p></li>
<li><p>标准回归的系数估计和 ANOVA table 都发生了改变，这是因为 SST
变成了 <span class="math inline">\(1\)</span>，ANOVA table
自然会变化。但是偏决定系数都是由偏相关系数直接决定的，所以没有变化。</p>
<p>类似地，如果只对 <span class="math inline">\(X\)</span>
做变换而不改变 <span class="math inline">\(Y\)</span>，会有系数发生改变，但是 ANOVA table
和偏决定系数都不变。</p></li>
</ul>
<h2 id="suppressor-variable">Suppressor Variable</h2>
<p>如果有 <span class="math inline">\(SSR(X_2 |X_1) &gt;
SSR(X_2)\)</span>，也就是 <span class="math inline">\(X_1\)</span>
在模型里的时候会让 <span class="math inline">\(X_2\)</span> 更显著，则称
<span class="math inline">\(X_1\)</span> 是一个 suppressor
variable。原理是 <span class="math inline">\(X_1\)</span>
可以帮助解释一部分 <span class="math inline">\(X_2\)</span>
中的噪音，使得 <span class="math inline">\(X_2\)</span> 更显著。</p>
<p><span class="math inline">\(X_1\)</span>
本身未必是显著的，这也就说明了如果单一变量不显著的话也不能贸然扔掉，它可能会是一个抑制变量。更极端的情况下，假设
<span class="math inline">\(X_1\)</span> 和 <span class="math inline">\(Y\)</span> 完全无关，有 <span class="math inline">\(X_2\)</span> 在模型中不会改变 <span class="math inline">\(X_1\)</span> 的系数估计，但是会导致 <span class="math inline">\(R^2_{Y2|1} \neq R^2_{Y2}\)</span>。</p>
<p>一般来说脑补一下这种类似韦恩图的直观解释就好。</p>
<p><img src="https://s2.loli.net/2023/05/30/4hFWcryX6lbspt5.png" alt="venn.png"></p>
<h1 id="lecture-10">Lecture 10</h1>
<h2 id="multicollinearity">Multicollinearity</h2>
<p>关于多重共线性的一些研究，先考虑一些极端情况，然后观察多重共线性会导致什么后果。</p>
<h3 id="zero-collinearity">Zero Collinearity</h3>
<p>在几个解释变量完全没有共线性的情况称为正交设计，也就是说设计矩阵的各列之间是正交的。这是一个很好的情况，互相之间并不会干扰，有
<span class="math inline">\(X^TX = diag(||X_0||^2 ,
||X_1||^2,...,||X_{p-1}||^2)\)</span>，且有 <span class="math inline">\(b_j = \frac{X_j ^TY}{||X_j||^2}\)</span>，<span class="math inline">\(Var(b_j) =
\frac{\sigma^2}{||X_j||^2}\)</span>。</p>
<p>结果就是无论进多少变量都不会影响单个 <span class="math inline">\(b_j\)</span> 的估计，但是会影响到 <span class="math inline">\(MSE\)</span> 导致 p-value 的变化。与此同时 type I,
II, III ANOVA table 的结果是一样的，这是因为 extra sum of squares
就是单个变量能解释的方差。</p>
<p><img src="https://s2.loli.net/2023/05/31/g3Abuh529lnpV7r.png" alt="venn-for-orthogonal-design.png"></p>
<h3 id="linearly-dependent">Linearly Dependent</h3>
<p>一个比较极端的例子是完全线性相关，比如变量之间有 <span class="math inline">\(c_1 X_1 + ... +c_{p-1}X_{p-1}=c\)</span>
这样的关系，会导致设计阵不满秩无法求逆。从数学上来说只要去掉其中一个变量即可，但是在统计上未必合适的。</p>
<h3 id="multicollinearity-1">Multicollinearity</h3>
<p>正常一些的情况就是普通的多重共线性，从回归结果来看多重共线性的一大特征就是模型整体显著，但是没有一个变量是显著的。回归结果的显著性是代表每个变量最后一个进入模型时的显著性，也就是说明每个变量几乎都是可以被前面进入模型的变量表示出来的。多重共线性有以下危害：</p>
<ul>
<li><span class="math inline">\(X_1,X_2\)</span>
之间较大的多重共线性会导致对于单个变量的 <span class="math inline">\(Var(b_i)\)</span>
增大，但是仍然是无偏估计。这也就说明了，出现多重共线性时 <span class="math inline">\(b_i\)</span> 的方差很大，会导致 <span class="math inline">\(b_i\)</span>
的估计值并不准确。举个例子来说，有的时候理论上 <span class="math inline">\(X_i\)</span> 和 <span class="math inline">\(Y\)</span>
之间是正相关，但是得到的系数估计是负的，就有可能是因为 <span class="math inline">\(Var(b_i)\)</span>
过大导致一组数据得到的结果距离“真实值”有很大的偏差，甚至从正相关变成了负相关。</li>
<li>Type I SS 和 Type II SS
的结果可能是不同的，因为变量之间对方差解释有竞争，也可能有 suppressor
variable，进入模型的顺序在此时变得重要了起来，二者不同的结果可能导致判断上的问题。</li>
<li>两部分解释变量解释了同一部分的方差，导致模型解释能力下降。模型整体显著但每一个变量都不显著，很有可能是过拟合了。</li>
<li>从数学上来说 <span class="math inline">\(X^TX\)</span>
接近退化，求逆时导致数值误差增大。</li>
</ul>
<p><img src="https://s2.loli.net/2023/05/31/iNHqIePg5rVKTQM.png" alt="multicollinearity.png"></p>
<p>有一些弥补的方案，但是要视建立模型的目的而定：</p>
<ul>
<li><p>如果单纯是为了预测，其实增大模型的 sample size
是可以解决问题的</p></li>
<li><p>如果是为了解释性，需要做很多其他的努力，比如移除一些变量，对变量做变换，PCA
方法等等。</p>
<p>仍然存在很多问题，比如移除变量时万一移除了某个重要的类别型变量，可能会导致
Simpson's Paradox 出现，移除变量也会导致系数估计的方差减小，可能减小 MSE
但是会导致 bias 增大，但如果移除了一个重要的解释变量会导致它进入 error
term，进而导致 <span class="math inline">\(\sigma^2\)</span>
的估计增大，需要 trade-off；</p>
<p>做变换不一定能成功降低共线性还会造成解释上的困难，PCA
的解释性更差，等等。</p></li>
</ul>
<p>Multicollinearity 可能有以下来源：</p>
<ul>
<li>抽样时 <span class="math inline">\(X\)</span> 的区域太小</li>
<li>理论上两个变量就是相关的却一起放进了模型，比如家庭收入和房屋面积</li>
<li>使用多项式回归</li>
<li><span class="math inline">\(p&gt;n\)</span></li>
<li>某些变量彼此是受同一隐含的因素影响的，比如一些 time series data</li>
</ul>
<h2 id="polynomial-regression">Polynomial Regression</h2>
<p>多项式回归可能会导致很强的共线性，比如一个只取 <span class="math inline">\(0\)</span> 和 <span class="math inline">\(1\)</span> 的类别型变量取 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(x^2\)</span>进入模型就完全共线。</p>
<p>有一个弥补的方案就是使用 centered
data，为每一个变量减去一个均值，导致数值有正有负，再做非负的平方项就得到共线性不那么强的两个解释变量。实际上再进一步对数据做尺度上的标准化也可以，但是对系数估计没有任何影响。</p>
<p>对数据做中心化不会导致高阶项的系数改变，但有可能会导致低阶项的系数和
extra sum of squares
变化。另外如果显著性不随之变化的话也有可能是出现了正交设计的情况，需要按照结果分析。</p>
<h2 id="交互项">交互项</h2>
<p>模型中存在交互项的本质就是 <span class="math inline">\(X_i\)</span>
对于 <span class="math inline">\(Y\)</span> 的效应和回归系数估计都会受到
<span class="math inline">\(X_j\)</span> 的影响。</p>
<ul>
<li><p>如果 <span class="math inline">\(X_2\)</span> 是连续型变量而
<span class="math inline">\(X_1\)</span>
是类别型变量，回归模型中包含二者的交互项，例如：</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2
+ \beta_3 X_1X_2+\varepsilon\]</span></p>
<p>这就说明对于 group 1，也就是 <span class="math inline">\(X_1 =
0\)</span> 时模型是 <span class="math inline">\(Y = \beta_0+ \beta_2 X_2
+ \varepsilon\)</span>，对于 group 2 也就是 <span class="math inline">\(X_1 = 1\)</span> 时模型是 <span class="math inline">\(Y = (\beta_0 + \beta_1 )+(\beta_2 + \beta_3)X_2 +
\varepsilon\)</span>。希望检验的问题是 <span class="math inline">\(\beta_1,\beta_3\)</span> 是否为 <span class="math inline">\(0\)</span>
来查看两组回归线的斜率、截距之间是否存在差异。</p></li>
<li><p>如果二者都是连续型变量也是类似的情况，相比之下类别型变量和连续型变量的交互效应有显著的分组意义。</p></li>
</ul>
<h1 id="lecture-11">Lecture 11</h1>
<h2 id="模型选择方法">模型选择方法</h2>
<h3 id="一些没那么数学的">一些没那么数学的</h3>
<ul>
<li>喜闻乐见的穷举，可惜只能处理不超过 <span class="math inline">\(n=40\)</span> 个变量的情况</li>
<li>stepwise greedy method，不喜闻乐见的要写代码，理解 idea
就好（心虚</li>
</ul>
<h3 id="一些准则">一些准则</h3>
<p>假设可供选择的 explanatory variable 有 <span class="math inline">\(P-1\)</span> 个，从中选择 <span class="math inline">\(p-1\)</span> 个并进行判断。</p>
<ul>
<li><p>观察 <span class="math inline">\(R^2\)</span> 和 adjusted <span class="math inline">\(R^2\)</span>，取后者较大的模型</p></li>
<li><p>观察 mallow's <span class="math inline">\(C_p\)</span>，<span class="math inline">\(\Gamma_p = \frac{E(SSE(p))}{\sigma^2} - (n-2p)
\geq p\)</span>，实际上在操作中只能取 <span class="math inline">\(\hat
\Gamma_p = \frac{SSE(p)}{MSE(P)}-(n-2p)\)</span>。</p>
<ul>
<li>如果 <span class="math inline">\(\hat \Gamma_p \gg p\)</span>
说明存在显著的误差，可能遗漏了重要的变量没有进入模型</li>
<li>如果 <span class="math inline">\(\hat \Gamma_p \ll p\)</span>
说明过拟合了，导致 <span class="math inline">\(SSE(p) \ll
MSE(P)\)</span></li>
<li>如果 <span class="math inline">\(\hat \Gamma_p \approx p\)</span>
说明是 unbiased，取接近于 <span class="math inline">\(p\)</span> 的
mallow's <span class="math inline">\(C_p\)</span>
中最小的一个对应的模型。</li>
</ul>
<p>实际上理论的形式对于 <span class="math inline">\(P\)</span> 有 <span class="math inline">\(\Gamma_P =P\)</span>。</p></li>
<li><p><span class="math inline">\(AIC = n \log (\frac{SSE(p)}{n}) +
2p\)</span>，<span class="math inline">\(BIC = n \log (\frac{SSE(p)}{n})
+ (\log n)p\)</span>，二者都是最小值对应的模型最合适。</p>
<p>注意 BIC 实际上相比 AIC
加了一个更大的惩罚在模型的变量数上，更注重解释性；一般来说有 <span class="math inline">\(BIC&gt;AIC\)</span>。</p></li>
<li><p>Predicted Residual Error Sum of Squares：<span class="math inline">\(PRESS(p) = \sum_{i=1}^n (Y_i - \hat
Y_{i(-i)})^2\)</span>，实际上有 <span class="math inline">\(Y_i - \hat
Y_{i(-i)} = \frac{e_i}{1-h_{ii}}\)</span>。</p>
<p>取使得 <span class="math inline">\(PRESS(p)\)</span>
最小的模型。用来观察过拟合与否。</p></li>
<li><p>Prediction <span class="math inline">\(R_p^2\)</span>：<span class="math inline">\(R_p^2 =
1-\frac{PRESS}{SST}\)</span>，如果模型里噪音过大，则有 <span class="math inline">\(PRESS &gt;SST\)</span>，此时 <span class="math inline">\(R_p^2&lt;0\)</span> 也是可以取负值的。如果有 <span class="math inline">\(R_p^2 \ll R^2\)</span>
则也可能是过拟合了，即使有些独立变量是显著的。</p></li>
</ul>
<p>我们选择模型一般会考虑 adjusted <span class="math inline">\(R^2\)</span>，AIC 或者 mallow's <span class="math inline">\(C_p\)</span>。</p>
<h2 id="模型诊断">模型诊断</h2>
<h3 id="partial-regression-plots">Partial Regression Plots</h3>
<p>每个 <span class="math inline">\(X_i\)</span> 都可以做出一张 partial
regression plot，也即所谓的 AV-plot，实际上就是对 <span class="math inline">\(Y|X_{-i} \sim X_i|X_{-i}\)</span>
这两部分残差互相做回归得到的图，展示了完整模型中 <span class="math inline">\(Y \sim X_i\)</span>
之间的边际关系。也可以用来检测非线性关系、异方差问题和 outliers。</p>
<h3 id="studentized-residuals">Studentized Residuals</h3>
<p>补充在 Lecture 6 里了。</p>
<h3 id="assessing-outliers">Assessing Outliers</h3>
<p>有以下指标可以考虑：</p>
<ul>
<li><p>Difference caused to fitted values：<span class="math inline">\((DFFIT)_i = \hat Y_i - \hat Y_{i(-i)} =
\frac{h_{ii}}{1-h_{ii}} e_i\)</span></p></li>
<li><p>Studentized DFFIT：<span class="math inline">\((DFFITS)_i =
\frac{\hat Y_i - \hat Y_{i(-i)}}{\sqrt{MSE_{-i} h_{ii}}} = t_i
\sqrt{\frac{h_{ii}}{1-h_{ii}}}\)</span>。</p>
<p>对于不太大的数据量，如果 <span class="math inline">\(|DFFITS|&gt;1\)</span>
则认为是强影响力点，大数据量时认为 <span class="math inline">\(|DFFITS|&gt;2\frac{\sqrt p}{\sqrt n}\)</span>
是强影响力点。</p></li>
<li><p>Cook's Distance：<span class="math inline">\(D_i = \frac{e_i^2}{p
\cdot MSE} \frac{h_{ii}}{(1-h_{ii})^2}\)</span>，在 R
中认为某个点有强影响力的 threshold 是 <span class="math inline">\(0.5\)</span>，事实上如果一个点的 Cook's distance
分布距离其他点较远，就可以认为是强影响力点了。</p></li>
<li><p>Difference in Beta Estimates：<span class="math inline">\((DFBETAS)_{k(-i)} = \frac{b_k -
b_{k(-i)}}{\sqrt{MSE_{(-i)} c_{kk}}}\)</span>，其中 <span class="math inline">\(c_{kk}\)</span> 是 <span class="math inline">\((X^TX)^{-1}\)</span> 的第 <span class="math inline">\(k \times k\)</span> 个分量。</p>
<p>对于大的数据量，如果 <span class="math inline">\(|DFFITS|&gt;1\)</span>
则认为是对于回归系数估计值的强影响力点，不太大的数据量时认为 <span class="math inline">\(|DFFITS|&gt;\frac{2}{\sqrt n}\)</span>
是对于回归系数估计值的强影响力点。</p></li>
</ul>
<h3 id="multicollinearity-diagnose">Multicollinearity Diagnose</h3>
<p>有以下两个指标：</p>
<ul>
<li>Variance Inflation
Factor：多重共线性经常导致方差膨胀，这是一个表征的指标，<span class="math inline">\((VIF)_k = (1- R_k^2)^{-1}\)</span>，其中 <span class="math inline">\(R_k^2\)</span> 指的是将 <span class="math inline">\(X_k\)</span> 相对于其他 <span class="math inline">\(p-2\)</span> 个 explanatory variable
做回归得到的决定系数。如果 <span class="math inline">\((VIF)_k \gg
1\)</span> 则认为第 <span class="math inline">\(k\)</span> 个变量 <span class="math inline">\(X_k\)</span> 是会发生多重共线性的变量。</li>
<li>Tolerance：<span class="math inline">\(TOL =
1/VIF\)</span>，判断准则类似上述。</li>
</ul>
<h1 id="lecture-12">Lecture 12</h1>
<p>本节探讨 remedies for multiple linear
regression，主要是异方差情况和多重共线性。</p>
<h2 id="equal-variance-remedy">Equal Variance Remedy</h2>
<p>这一部分主要处理模型发生异方差问题的情况。</p>
<p>也就是说，实际上有 <span class="math inline">\(\varepsilon_1,
\varepsilon_2, ..., \varepsilon_n\)</span> 的方差不全为 <span class="math inline">\(\sigma^2\)</span>，记方差为 <span class="math inline">\(\sigma_1 ^2,\sigma_2 ^2 , ..., \sigma_n
^2\)</span>。此时的 likelihood function 是：</p>
<p><span class="math display">\[L(\beta_1,\beta_2) = \Pi_{i=1} ^n
f_i(\beta_1, \beta_2 |X_i,Y_i) = \Pi_{i=1}^n (\frac{1}{\sqrt{2\pi}
\sigma_i} exp(-\frac 1 2 (\frac{Y_i - \beta_0 -\beta_1
X_i}{\sigma_i})^2)) \]</span></p>
<p>可以看到 ordinary least square 条件得到的 <span class="math inline">\(b_0 = \hat \beta_0,b_1 = \hat \beta_1\)</span>
不再满足 MLE 的条件。但如果是在不知异方差的情况下仍然使用 OLS 或者 MLE
条件得到的参数估计，仍然是可以满足无偏性的，但不满足最小方差性质。</p>
<h3 id="weighted-regression">Weighted Regression</h3>
<p>简单来说，通过选取 <span class="math inline">\(\{ w_i =
\frac{1}{\sigma_i ^2} \}_{i=1} ^n\)</span> 作为权重，考虑 <span class="math inline">\(\Sigma_{i=1} ^n w_i e_i^2 = \Sigma_{i=1} ^n
(\frac{Y_i - \beta_0 - \beta_1 X_{i,1} - ... - \beta_{p-1}
X_{i,p-1}}{\sigma_i})^2\)</span> 的最小性问题。</p>
<ul>
<li><p>最简单的情况，<span class="math inline">\(\sigma_1,...,\sigma_n\)</span> 均已知，则有 <span class="math inline">\(w_i = \frac{1}{\sigma_i ^2}\)</span>，</p>
<p>取 <span class="math inline">\(W = \begin{bmatrix} \frac{1}{\sigma_1
^2} &amp; &amp; &amp; \\ &amp;...&amp; &amp; \\ &amp; &amp; &amp;
\frac{1}{\sigma_n^2} \end{bmatrix}\)</span> 使得回归问题变为 <span class="math inline">\(W^{\frac 1 2} Y = W^{\frac 1 2} X \beta + W^{\frac
1 2} \varepsilon\)</span>，记 <span class="math inline">\(Y^* = W^{\frac
1 2} Y, X^* = W^{\frac 1 2} X, \varepsilon^* = W^{\frac 1 2}
\varepsilon\)</span>，由于 <span class="math inline">\(Var(\varepsilon^*) = I_{n\times n}\)</span>
满足同方差条件，因此新的回归问题是符合条件的。</p>
<p>注意新的回归问题 <span class="math inline">\(Y^* = X^* \beta +
\varepsilon^*\)</span> 事实上没有改变系数 <span class="math inline">\(\beta\)</span>，但系数估计 <span class="math inline">\(b_w = (X^TWX)^{-1} (X^TWY)\)</span>
是改变了的，这是正常现象，因为对这一问题做 remedy
的主要原因就是假设不满足，导致按照 OLS 做出的系数估计不准确，因此
weighted regression 做出的修正也是相对于原系数 <span class="math inline">\(\beta\)</span> 的。</p>
<p><span class="math inline">\(b_w\)</span> 仍然是无偏估计，也保证 <span class="math inline">\(Var(b_w) = (X^TWX)^{-1}\)</span>
是最小方差。</p></li>
<li><p>稍微复杂一点的情况，虽然 <span class="math inline">\(\sigma_i^2\)</span> 未知但 <span class="math inline">\(\sigma^2_i / \sigma_j ^2\)</span> 均已知，取 <span class="math inline">\(w_i = \frac{\sigma_1 ^2}{\sigma_i^2 }\)</span>
即可。于是有：</p>
<p>取 <span class="math inline">\(W = \sigma_1 ^2 \begin{bmatrix}
\frac{1}{\sigma_1 ^2} &amp; &amp; &amp; \\ &amp;...&amp; &amp; \\ &amp;
&amp; &amp; \frac{1}{\sigma_n^2} \end{bmatrix}\)</span> 使得回归问题变为
<span class="math inline">\(W^{\frac 1 2} Y = W^{\frac 1 2} X \beta +
W^{\frac 1 2} \varepsilon\)</span>，记 <span class="math inline">\(Y^* =
W^{\frac 1 2} Y, X^* = W^{\frac 1 2} X, \varepsilon^* = W^{\frac 1 2}
\varepsilon\)</span>，由于 <span class="math inline">\(Var(\varepsilon^*) = \sigma_1 ^2 I_{n\times
n}\)</span> 满足同方差条件，因此新的回归问题是符合条件的。</p>
<p>新的回归系数估计是 <span class="math inline">\(b_w =
(X^TWX)^{-1}(X^TWY)\)</span>，<span class="math inline">\(Var(b_w) =
\sigma_1^2(X^TWX)^{-1}\)</span>，由此还可以做出对 <span class="math inline">\(\sigma_1\)</span> 的参数估计，<span class="math inline">\(\hat \sigma_1 ^2 = MSE_{wls} = \frac{\Sigma(Y^*_i
- \hat Y^*_i)^2}{n-p}= \frac{\Sigma w_i(Y_i - \hat Y_i)^2}{n-p} =
\frac{\Sigma w_i e_i ^2}{n-p}\)</span>，其中 <span class="math inline">\(w_i =
\frac{\sigma_1^2}{\sigma_i^2}\)</span>。</p></li>
<li><p>一般情况下 <span class="math inline">\(\{ \sigma_i \}\)</span>
是完全未知的，我们是在模型诊断中发现异方差的现象，因此不可能直接通过方差值推权重系数。这个时候一般有两种选择：</p>
<ul>
<li><p>重复试验取 <span class="math inline">\(Y_i\)</span> 的方差估计
<span class="math inline">\(s_i^2\)</span>，于是权重系数为 <span class="math inline">\(w_i = \frac{1}{s_i ^2}\)</span>。</p></li>
<li><p>先对 <span class="math inline">\(Y\sim X\)</span> 进行 OLS
回归，取出此时的 residual <span class="math inline">\(\{e_i \}\)</span>
作为 <span class="math inline">\(\{ \sigma _i \}\)</span> 的估计，取
<span class="math inline">\(w_i = \frac{1}{e_i ^2}\)</span>
作为权重即可。</p>
<p>效果不明显时多迭代几次。</p></li>
</ul></li>
</ul>
<p>在观察 weighted least square 和 ordinary least square
模型差别时，注意：</p>
<ul>
<li><span class="math inline">\(R^2\)</span> 以及 adjusted <span class="math inline">\(R^2\)</span> 的数值差别没有很强的意义，WLS
情况下原始数据 <span class="math inline">\(\{ Y_i \}\)</span>
已经发生了变化，实际上 SST 也已经变了，没有什么比较的意义。</li>
<li>需要关注的点是 residual standard error，越接近 <span class="math inline">\(1\)</span> 越说明异方差的调整是成功的。</li>
<li>有时会观察到 WLS 情况下 <span class="math inline">\(\hat
\sigma^2\)</span> 在减小，似乎数据的分散程度在减小，这是因为 MSE
在减小。但是 WLS 和 OLS 情况下的 MSE 和 MST 都没有比较的意义，因此 <span class="math inline">\(\hat \sigma\)</span>
的变化也没有研究的价值。唯一确定的是它会接近于 <span class="math inline">\(1\)</span>，这一点可以证明异方差的调整效果是成功的。</li>
</ul>
<h2 id="multicollinearity-remedy">Multicollinearity Remedy</h2>
<p>如果存在多重共线性，主要发生的问题是 <span class="math inline">\(X^TX\)</span>
求逆是一个病态的数值问题，误差很大。实际上极端来说如果存在完全共线性，<span class="math inline">\(X^TX\)</span> 会退化为不满秩的情况。</p>
<p>可以用 ridge regression 对多重共线性进行弥补。</p>
<h3 id="ridge-regression">Ridge Regression</h3>
<p>主要的 idea 是如果 <span class="math inline">\((X^TX)\)</span>
接近于不满秩，则在参数估计中将其改变为 <span class="math inline">\((X^TX
+\lambda I)\)</span>，<span class="math inline">\(\lambda\)</span>
是待取的参数。对于矩阵的对角元进行改变如同突起的山脊，因此得名岭回归。</p>
<p>Ridge regression 的本质是对优化问题进行了修改。OLS
中的优化问题是求使得 <span class="math inline">\((Y-X\beta)^T
(Y-X\beta)\)</span> 最小的 <span class="math inline">\(\beta\)</span>，ridge regression 中将 <span class="math inline">\(|| \beta ||_2 ^2 = \Sigma_{i=0} ^{p-1}
\beta_i^2\)</span> 加入了优化，对 <span class="math inline">\(\beta\)</span> 的长度（事实上应该称之为 2-
范数）做惩罚。因此，实际上是求使得 <span class="math inline">\((Y-X
\beta )^T (Y-X \beta) + \lambda \Sigma_{i=0} ^{p-1}\beta_i ^2\)</span>
最小的 <span class="math inline">\(\hat \beta = (X^TX + \lambda I)^{-1}
X^TY\)</span>，这使得参数估计 <span class="math inline">\(b = \hat
\beta\)</span> 呈现出比 OLS 下长度和方差都更小的特征。</p>
<p>在实际应用中，需要通过确定最佳的 <span class="math inline">\(\lambda\)</span> 从而得到合适的参数估计，一般是对
<span class="math inline">\(\lambda\)</span> 取一个 sequence
进行尝试。如果发现某个 explanatory variable 的系数在 <span class="math inline">\(\lambda\)</span> 增大时很快下降到 <span class="math inline">\(0\)</span>，实际上它很有可能是不需要进入模型的。</p>
<p>应用岭回归来弥补模型的多重共线性的时候，既是为了消除共线性，也是在牺牲一些
<span class="math inline">\(\hat \beta\)</span>
的无偏性来换取更小的方差。</p>
<h3 id="lasso-elastic-net">LASSO &amp; Elastic Net</h3>
<p>LASSO 中把惩罚的 <span class="math inline">\(\beta\)</span>
长度替换为了 <span class="math inline">\(\beta\)</span> 的 1-
范数，弹性网络则是对 LASSO 和 ridge regression 进行了结合。</p>
<p>本质上都是 Bayesian modes。</p>
<h2 id="influencial-cases-remedy">Influencial Cases Remedy</h2>
<p>更改一些更 robust 的优化模型，例如 least absolute deviation 和 least
median of squares，缺点是算起来会比较困难。</p>
<p>或者考虑非参数模型。</p>
<h2 id="nonlinearity-remedy">Nonlinearity Remedy</h2>
<p>考虑局部多项式回归/局部回归，总之是对数据进行分块，所谓的
lowess。</p>
<h1 id="lecture-13">Lecture 13</h1>
<h2 id="one-factor-anova">One Factor ANOVA</h2>
<p>（从生统笔记复制来的）</p>
<p>首先给出一个希望做检验的场景：<span class="math inline">\(n_T\)</span> 个实验对象被分成 <span class="math inline">\(r\)</span> 组，每组有 <span class="math inline">\(n_i\)</span> 个实验对象，有 <span class="math inline">\(n_T = \Sigma_{i=1} ^r n_i\)</span>。由此我们得到
<span class="math inline">\(n_T\)</span> 个数据 <span class="math inline">\(Y_{ij}\)</span>，<span class="math inline">\(i\)</span> 表示组别，<span class="math inline">\(1\leq i \leq r\)</span>，<span class="math inline">\(j\)</span> 表示在某一组内的编号，<span class="math inline">\(1 \leq j \leq n_i\)</span>。</p>
<h3 id="cell-means-model">Cell means model</h3>
<ul>
<li><p>模型假设是 <span class="math inline">\(Y_{ij} = \mu_i +
\varepsilon_{ij}\)</span>。</p>
<p>其中，<span class="math inline">\(\mu_i\)</span> 是第 <span class="math inline">\(i\)</span> 组的理论均值，<span class="math inline">\(\varepsilon _{ij}\)</span> i.i.d. <span class="math inline">\(\sim
N(0,\sigma^2)\)</span>。注意到在这一模型假设中有 <span class="math inline">\(r+1\)</span> 个参数，分别是 <span class="math inline">\(\mu_1,\mu_2,...,\mu_r,\sigma^2\)</span>，我们需要用得到的数据来对这些未知参数进行估计。考虑一些统计量作为参数的估计量：</p>
<p><span class="math display">\[\bar{Y}_{i.} = \frac{1}{n_i} \Sigma_{j=1
}^{n_i} Y_{ij}  = \hat \mu _i\]</span></p>
<p><span class="math display">\[\bar{Y}_{..} = \frac{1}{n_T}
\Sigma_{i=1}^r \Sigma_{j=1} ^{n_i} Y_{ij} = \frac{1}{n_T} \Sigma_{i=1}^r
n_i \bar{Y}_{i.}\]</span></p>
<p><span class="math display">\[s_i ^2 = \Sigma_{j=1} ^{n_i} (Y_{ij} -
\bar Y_{i.})^2 / (n_i -1) \]</span></p>
<p><span class="math display">\[s^2 =\Sigma_{i=1} ^r \Sigma_{j=1} ^{n_i}
(Y_{ij} - \bar Y_{i.})^2  =\frac{1}{n_T- r} \Sigma_{i=1}^r (n_i -1) s_i
^2 = \hat \sigma^2\]</span></p></li>
<li><p>在这一模型中，我们关注的假设检验是 <span class="math inline">\(i\)</span> 组实验之间是否存在差异，假设检验表示为
<span class="math inline">\(H_0 : \mu_1= \mu_2 = ... = \mu_r =
\mu\)</span>，对应的备择假设即为 <span class="math inline">\(\{ \mu_i
\}_{i=1} ^r\)</span> 中存在不同的项。检验最经典的方法即为
ANOVA，analysis of variance。核心是以下的分解：</p>
<p><span class="math display">\[\begin{aligned} SSTO = \Sigma_i
\Sigma_j(Y_{ij} - \bar Y_{..})^2 &amp;= \Sigma_i \Sigma_j (Y_{ij} - \bar
Y_{i.} +\bar Y_{i.} - \bar Y_{..})^2 \\&amp; = \Sigma_i n_i (\bar Y_{i.}
- \bar Y_{..})^2 + \Sigma_i \Sigma_j (Y_{ij} - \bar Y_{i.})^2 \\
&amp;=SSTR + SSE \end{aligned}\]</span></p>
<p>可以观察到，<span class="math inline">\(SSTR\)</span>
是组间差距，体现了不同组别之间的差别，<span class="math inline">\(SSE\)</span>
是组内差距，体现了同一组内各数据的偏差。注意 <span class="math inline">\(SSTR\)</span> 的自由度是 <span class="math inline">\(r-1\)</span>，<span class="math inline">\(SSE\)</span> 的自由度是 <span class="math inline">\(n_T - r\)</span>，<span class="math inline">\(SSTO\)</span> 的自由度是 <span class="math inline">\(n_T - 1\)</span>。</p>
<p>两个统计量的期望是 <span class="math inline">\(\mathbb E(MSE) =
\sigma^2\)</span>，<span class="math inline">\(\mathbb E(MSTR) =
\sigma^2 +\frac{\Sigma_i n_i (\mu _i -\mu_.)^2}{r-1}\)</span>，其中
<span class="math inline">\(\mu_. = \frac{\Sigma_i n_i
\mu_i}{n_T}\)</span>。</p>
<p>在 <span class="math inline">\(H_0\)</span> 成立时，<span class="math inline">\(\frac{SSE}{\sigma^2} \sim \chi^2 _{n_T -
r}\)</span>，<span class="math inline">\(\frac{SSTR}{\sigma^2} \sim
\chi^2_{r-1}\)</span>。因此 <span class="math inline">\(F=\frac{MSTR}{MSE} \sim F_{r-1,n_T -r}\)</span>
作为最终的检验统计量。</p>
<p>当 <span class="math inline">\(F^* &gt; F(1-\alpha , r-1 , n_T
-r)\)</span> 时拒绝原假设，否则接受；<span class="math inline">\(Power =
P(F^* &gt; F(1-\alpha , r-1,n_T -r)| \delta)\)</span>，其中 <span class="math inline">\(\delta\)</span> 是一个非中心偏移量，<span class="math inline">\(\delta = \frac{1}{\sigma} \sqrt{\frac 1 r \Sigma_i
n_i (\mu_i - \mu_.)^2}\)</span>。</p></li>
<li><p>也可以作为一个线性回归的问题来看待，design matrix 是 $ X =</p>
<span class="math display">\[\begin{bmatrix} 1 &amp; 0  &amp; 0 &amp;
... &amp; 0 \\ 0 &amp; 1  &amp; 0 &amp; ... &amp; 0 \\ ...
&amp;...&amp;...&amp;...&amp;... \\ 0 &amp; 0 &amp; 0 &amp; ... &amp; 1
\end{bmatrix}\]</span>
<p>$，系数向量是 <span class="math inline">\(\mu = \begin{bmatrix} \mu_1
\\ \mu_2 \\ ... \\ \mu_r \end{bmatrix}\)</span>，因此整体的回归方程是 $
Y = X + $，注意这个回归问题是<strong>强迫过原点</strong>的。</p></li>
</ul>
<h3 id="factor-effects-model">Factor Effects Model</h3>
<ul>
<li><p>Factor Effects Model 是 Cell Means Model
的一个重新参数化的结果。模型假设是 <span class="math inline">\(Y_{ij} =
\mu + \tau_i + \varepsilon_{ij}\)</span>。</p>
<p>其中，<span class="math inline">\(\mu_i\)</span>
是整体的理论均值，<span class="math inline">\(\varepsilon _{ij} i.i.d.
\sim N(0,\sigma^2)\)</span>。它的参数比 cell mean model 多一个，分别是
<span class="math inline">\(\mu , \tau_1 , ..., \tau_r ,
\sigma^2\)</span>，但是自由度是相同的，因为 <span class="math inline">\(\{ \tau _i \}_{i=1}^r\)</span> 存在一个约束 <span class="math inline">\(\Sigma_{i=1}^r \tau_i =
0\)</span>，如果没有这个约束会导致存在多组解。考虑一些统计量作为参数的估计量：</p>
<p><span class="math display">\[\bar{Y}_{i.} = \frac{1}{n_i} \Sigma_{j=1
}^{n_i} Y_{ij}  = \hat \tau _i + \hat \mu\]</span></p>
<p><span class="math display">\[\bar{Y}_{..} = \frac{1}{n_T}
\Sigma_{i=1}^r \Sigma_{j=1} ^{n_i} Y_{ij} = \frac{1}{n_T} \Sigma_{i=1}^r
n_i \bar{Y}_{i.} = \hat{\mu}\]</span></p>
<p><span class="math display">\[s_i ^2 = \Sigma(Y_{ij} - \bar Y_{i.})^2
/ (n_i -1) \]</span></p>
<p><span class="math display">\[s^2 = \frac{1}{n_T- r} \Sigma_{i=1}^r
(n_i -1) s_i ^2 = \hat \sigma^2\]</span></p></li>
<li><p>在这一模型中，我们关注的假设检验仍然是 <span class="math inline">\(i\)</span> 组实验之间是否存在差异，假设检验表示为
<span class="math inline">\(H_0 : \tau_1= \tau_2 = ... = \tau_r =
0\)</span>，对应的备择假设即为 <span class="math inline">\(\{ \mu_i
\}_{i=1} ^r\)</span> 中存在不同的项。factor effects model
在参数的含义上比 cell mean model 更清晰。</p></li>
<li><p>可以作为一个线性回归的问题来看待，design matrix 是 $ X =</p>
<span class="math display">\[\begin{bmatrix} 1 &amp; 1  &amp; 0 &amp;
... &amp; 0 \\ 1 &amp; 0  &amp; 1 &amp; ... &amp; 0 \\ ...
&amp;...&amp;...&amp;...&amp;... \\ 1 &amp; -1 &amp; -1 &amp; ... &amp;
-1 \end{bmatrix}\]</span>
<p>$，系数向量是 <span class="math inline">\(\mu = \begin{bmatrix} \mu
\\ \tau_1 \\ \tau_2 \\ ... \\ \tau_{r-1}
\end{bmatrix}\)</span>，因此整体的回归方程是 $ Y = X +
$，注意这个回归问题的截距就是 <span class="math inline">\(\mu\)</span>，不强迫过原点，相比 cell mean model
算是做了一点点优化。</p></li>
</ul>
<h3 id="example">Example</h3>
<ul>
<li><p>在做回归之前要注明哪些变量是 factor：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> data<span class="operator">$</span>design <span class="operator">=</span> factor<span class="punctuation">(</span>data<span class="operator">$</span>design<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="operator">&gt;</span> fit <span class="operator">=</span> lm<span class="punctuation">(</span>cases <span class="operator">~</span> design<span class="punctuation">,</span> data <span class="operator">=</span> data<span class="punctuation">)</span></span><br><span class="line"><span class="operator">&gt;</span> summary<span class="punctuation">(</span>fit<span class="punctuation">)</span></span><br></pre></td></tr></table></figure></li>
<li><p>方便查看 <span class="math inline">\(\hat \mu_i\)</span>
的命令是过原点回归，但查看 MSR 的方式是不过原点回归。</p></li>
<li><p><span class="math inline">\(Std.Error_i ^2 = Var(\hat \mu_i) =
Var(\frac{\Sigma_{j=1}^{n_i} Y_{ij}}{n_i}) = \frac{\hat \sigma^2}{n_i}=
\frac{s^2}{n_i}\)</span></p>
<p><span class="math inline">\(sd_i ^2 = s_i ^2\)</span>，由此计算出
<span class="math inline">\(s^2\)</span> 后再得到每个 <span class="math inline">\(Std.Error\)</span> 的值是 <span class="math inline">\(\frac{s}{\sqrt n_i}\)</span>。</p></li>
</ul>
<h2 id="inference-on-one-way-anova">Inference on One-Way ANOVA</h2>
<h3 id="confidence-interval-for-mu_i">Confidence Interval for <span class="math inline">\(\mu_i\)</span></h3>
<p>事实上这只是个理论上可做的问题而已，现实中不会对 <span class="math inline">\(\mu_i\)</span> 做推断，我们关注的是 <span class="math inline">\(\mu_i\)</span> 之间的差异。</p>
<p>由于 <span class="math inline">\(\bar Y_{i.} \sim N(\mu_i , \sigma^2
/ n_i)\)</span>，因此 <span class="math inline">\(\mu_i\)</span> 的
pooled confidence interval 是 <span class="math inline">\((\bar Y_{i.} -
t_c \frac{s}{\sqrt n_i} , \bar Y_{i.} + t_c \frac{s}{\sqrt
n_i})\)</span>，其中 <span class="math inline">\(t_c =
t(1-\frac{\alpha}{2}, n_T -r)\)</span>。注意其中的 <span class="math inline">\(s^2\)</span> 在上面已经有定义，实际上就是
SSE。</p>
<p>当然也可以认为是 <span class="math inline">\((\bar Y_{i.} - t_c s_i ,
\bar Y_{i.} + t_c s_i)\)</span>，<span class="math inline">\(s_i\)</span> 是每个 <span class="math inline">\(\mu_i\)</span> 对应的 <span class="math inline">\(sd_i\)</span>。但是这样得到的置信区间一般来说较宽，准确性不如
pooled confidence interval，我们不太会采用。</p>
<p>实际上这样做 t-test 的话 family-wise error rate
很大，即使做出显著的效果也很有可能是发生了 Type I Error。</p>
<h3 id="bonferroni-confidence-intervals-for-mu_i">Bonferroni Confidence
Intervals for <span class="math inline">\(\mu_i\)</span></h3>
<p>想要同时估计所有的 <span class="math inline">\(\mu_i\)</span>
的时候可以采用 Bonferroni method，但是也有明显的缺点是一旦 factor level
<span class="math inline">\(r\)</span> 较大，就会导致每个 <span class="math inline">\(\mu_i\)</span> 都不显著，置信区间的 level of
significance 只有 <span class="math inline">\(\alpha /
2r\)</span>，几乎是无效的。</p>
<p>同样是 t-test，但过于保守了。对于较小的 <span class="math inline">\(r\)</span> 可以进行尝试。</p>
<h3 id="test-difference-in-means">Test Difference in Means</h3>
<p>由于 <span class="math inline">\(\bar Y_{i.} - \bar Y_{j.} \sim
N(\mu_i - \mu_k , \frac{\sigma^2}{n_i} + \frac{\sigma
^2}{n_j})\)</span>，<span class="math inline">\(\mu_i - \mu_k\)</span>
的 confidence interval 是 <span class="math inline">\((\bar Y_{i.} -
\bar Y_{j.}-t_c s(\bar Y_{i.} - \bar Y_{k.}), \bar Y_{i.} - \bar
Y_{j.}+t_c s(\bar Y_{i.} - \bar Y_{k.}))\)</span>，其中 <span class="math inline">\(s(\bar Y_i. - \bar Y_k.) = \sqrt{\frac{\hat
\sigma^2}{n_i} + \frac{\hat \sigma^2}{n_j}} = s \sqrt{\frac{1}{n_i} +
\frac{1}{n_j}}\)</span>，<span class="math inline">\(t_c\)</span>
是和检验方法有关的常数。</p>
<p>由于一共有 <span class="math inline">\(r\)</span> 个 mean <span class="math inline">\(\mu_i\)</span>，所以一共要做 <span class="math inline">\(\frac{r(r-1)}{2}\)</span>
次检验来确定两两之间有无差异。</p>
<ul>
<li>Tukey's HSD Method: 使用 q-test，取 <span class="math inline">\(t_c
= \frac{\bar y _{max} - \bar y_{min}}{\sqrt 2 s /\sqrt
n}\)</span>；适用于两两检验。</li>
<li>Scheffe's Method: 使用 F-test，取 <span class="math inline">\(t_c =
\sqrt{(r-1)F(1-\alpha, r-1, n_T
-r)}\)</span>，适用于线性组合的对照（contrast，见下）。实际上也过于保守了，导致
power 比较低。</li>
</ul>
<h2 id="contrast">Contrast</h2>
<h3 id="concept">Concept</h3>
<p>关于对照的具体定义是，取一组均值为 <span class="math inline">\(0\)</span> 的常数作为权重，即为 <span class="math inline">\(\Sigma_{i=1} ^r c_i =0\)</span>，此时研究 <span class="math inline">\(L = \Sigma_{i=1} ^r c_i \mu_i\)</span>
的推断。</p>
<p>注意到 <span class="math inline">\(\hat L = \Sigma c_i \bar Y_i .
\sim N(L, Var(\hat L))\)</span>，其中 <span class="math inline">\(Var(\hat L) = \Sigma c_i ^2 Var(\bar Y_i
.)\)</span>，<span class="math inline">\(\hat Var (\hat L) = MSE \Sigma(
c^2_i / n_i)\)</span>。</p>
<p>test statistic 是 <span class="math inline">\(T= \frac{\hat L -
L_0}{\sqrt{\hat Var(\hat L)}} \sim t(n_T-r)\)</span>。例如在 <span class="math inline">\(H_0 : L =0\)</span> 下有 <span class="math inline">\(T = \frac{\Sigma c_i \bar Y_i .}{\sqrt{MSE \Sigma
c^2_i / n_i}} \sim t(n_T -r)\)</span>，于是 <span class="math inline">\(T^2 = \frac{(\Sigma c_i \bar Y_i.)^2}{MSE \Sigma
c_i^2 /n_i} = \frac{SSC/1}{MSE} \sim F(1,n_T-r)\)</span>，其中定义 <span class="math inline">\(SSC = (\Sigma c_i \bar Y_i.)^2 / \Sigma
(c_i^2/n_i)\)</span>，称为 sum of contrast。</p>
<h3 id="multiple-contrasts">Multiple Contrasts</h3>
<p>可以利用 R 同时检验若干组 contrast，比如同时检验 <span class="math inline">\(\mu_1 = \mu_2, \mu_3 = \mu_2 , \mu_1 =( \mu_1 +
\mu_2+\mu_3 )/ 3\)</span></p>
<p>实际上 linear hypothesis test 和 multiple comparison
的主要差别在于自由度，比如说对于 <span class="math inline">\(\mu_1 =
\mu_2 = \mu_3\)</span> 做检验，前者会将其拆成两个两两检验，自由度是
<span class="math inline">\(2\)</span>，后者会作为一个整体的
contrast，自由度是 <span class="math inline">\(1\)</span>。</p>
<h1 id="lecture-14">Lecture 14</h1>
<p>依旧来自生统概论的笔记。</p>
<h2 id="two-way-anova">Two-Way ANOVA</h2>
<p>首先给出一个希望做检验的场景：<span class="math inline">\(nab\)</span> 个实验对象被分成 <span class="math inline">\(a\times b\)</span> 组，每组有 <span class="math inline">\(n\)</span> 个实验对象。第 <span class="math inline">\(ij\)</span> 组的实验条件是 <span class="math inline">\(A\)</span> 因素的等级为 <span class="math inline">\(i\)</span>，<span class="math inline">\(B\)</span>
因素的等级为 <span class="math inline">\(j\)</span>，其中有 <span class="math inline">\(1 \leq i \leq a, 1\leq j \leq
b\)</span>。由此我们得到 <span class="math inline">\(nab\)</span> 个数据
<span class="math inline">\(Y_{ijk}\)</span>，<span class="math inline">\(i\)</span> 表示以 <span class="math inline">\(A\)</span> 因素分类的组别，<span class="math inline">\(j\)</span> 表示以 <span class="math inline">\(B\)</span> 因素分类的组别，<span class="math inline">\(k\)</span> 表示在某一组内的编号，<span class="math inline">\(1 \leq k \leq n\)</span>。</p>
<p>每一组都是 <span class="math inline">\(n\)</span> 个人，这是一个
balanced design。</p>
<h3 id="cell-mean-model">Cell Mean Model</h3>
<p>模型假设是 <span class="math inline">\(Y_{ijk} = \mu_{ij} +
\varepsilon_{ijk}\)</span>。</p>
<p>其中，<span class="math inline">\(\mu_{ij}\)</span> 是第 <span class="math inline">\({i \times j}\)</span> 水平的均值，<span class="math inline">\(\varepsilon_{ijk}\)</span> i.i.d. <span class="math inline">\(\sim N(0,\sigma^2)\)</span>，模型中实际上有 <span class="math inline">\(ab+1\)</span> 个未知参数需要估计。</p>
<p><span class="math inline">\(\mu_{ij}\)</span> 的估计量是 <span class="math inline">\(\bar Y_{ij.} = \sum_k Y_{ijk} /n\)</span>，对于
<span class="math inline">\(i \times j\)</span> 水平的方差估计是 <span class="math inline">\(s_{ij}^2 = \sum_k (Y_{ijk} - \bar Y_{ij.})^2 /
(n-1)\)</span>。但是想要估计 <span class="math inline">\(\sigma^2\)</span> 时必须要将所有的数值加权 pool
起来做估计，是 <span class="math inline">\(s^2 = \sum_{ij} (n_{ij}-1)
s_{ij} ^2 / \sum_{ij} (n_{ij}-1)\)</span>，注意到如果是 balanced test
的情况实际上就是 <span class="math inline">\(s^2 = \sum_{ij} s^2_{ij} /
ab\)</span> 直接做平均的结果。更倾向于 pooled <span class="math inline">\(s^2\)</span>
是因为自由度更大，数据利用更充分。</p>
<p>直接通过看图来观察两个因子之间是否存在交互效应、单因子是否显著这件事的时候，比较经典的情况就是以下两种：</p>
<p><img src="https://s2.loli.net/2023/06/02/Hb1oPSrA2IJFcmC.png" alt="cell-mean-model.png"></p>
<p>上面第一张图中可以发现两条回归线之间存在斜率的差异，说明 B 因子对于 A
因子的效果存在影响，也就是存在交互效应；在 <span class="math inline">\(b_2\)</span> level 上 A 因子是不显著的，但在 <span class="math inline">\(b_1\)</span> level 上 A 因子显著；同理在 <span class="math inline">\(a_1\)</span> level 上 B 因子不显著，但在 <span class="math inline">\(a_2\)</span> level 上 B
因子显著。实际上在这个情况下交互效应显著，主效应虽然显著但也没有太大意义了，不过想要解释也是可以的，可以认为
A 因子带来的效应至少不是负效应。</p>
<p>第二张图里更有两条回归线交叉，存在斜率的差异，交互效应显著；但主效应此时可能无法解释，尤其是如果两条回归线完全交叉成
<span class="math inline">\(\times\)</span> 形状，A 因子会在不同的 B
因子条件下起到相反的作用。所以一般是认为交互效应显著时主效应显著，但没有解释意义。显著性和解释性之间无关。</p>
<p>在读 R code 的时候直接把所有的 estimation 读作 factor effect model
的系数，再代回就可以理解系数的来源了。这一部分在 factor effects model
里详述。</p>
<h3 id="factor-effects-model-1">Factor Effects Model</h3>
<p>模型假设是 <span class="math inline">\(Y_{ijk} = \mu +\alpha_i +
\beta_j +(\alpha \beta)_{ij} + \varepsilon_{ijk}\)</span>。</p>
<p>其中，<span class="math inline">\(\mu\)</span> 是整体的均值，<span class="math inline">\(\alpha_i\)</span> 代表只和等级为 <span class="math inline">\(i\)</span> 的因素 <span class="math inline">\(A\)</span> 有关的变化，<span class="math inline">\(\beta_j\)</span> 表示只和等级为 <span class="math inline">\(j\)</span> 的因素 <span class="math inline">\(B\)</span> 有关的变化，<span class="math inline">\((\alpha \beta)_{ij}\)</span>
表示和两个因素同时相关的变化，相当于一个交叉项。</p>
<p>这里面有 <span class="math inline">\((a+1)(b+1)\)</span>
个参数，对其也有一定的约束：<span class="math inline">\(\Sigma_i
\alpha_i = \Sigma_j \beta_j = \Sigma_i (\alpha \beta)_{ij} = \Sigma_j
(\alpha \beta)_{ij} =0\)</span>，事实上有 <span class="math inline">\(a+b-1\)</span> 个关于交互效应的约束，有 <span class="math inline">\(2\)</span> 个关于单因子效应的约束，实际上包含
<span class="math inline">\(\sigma^2\)</span> 后仍然是有 <span class="math inline">\(ab+1\)</span>
个参数。做一些其他的参数假设，让参数含义更清晰：</p>
<p><span class="math display">\[\mu_{i.} = \alpha_i
+\mu_{..}\]</span></p>
<p><span class="math display">\[\mu_{.j} = \beta _j +
\mu_{..}\]</span></p>
<p><span class="math display">\[\mu_{ij} = \mu_{..} +\alpha_i + \beta_j
+(\alpha \beta)_{ij}\]</span></p>
<p>如果 <span class="math inline">\((\alpha \beta)_{ij}=0\)</span>
则说明因素 <span class="math inline">\(A,B\)</span>
之间不存在相互作用的关系，这样的模型称为 additive model。</p>
<p>考虑一些统计量作为参数的估计量：</p>
<p><span class="math display">\[\bar{Y}_{ij.} = \frac{1}{n} \Sigma_{k=1
}^{n} Y_{ijk}  = \hat \mu_{ij} = \hat \mu + \hat \alpha _i + \hat
\beta_j + \hat{(\alpha \beta)}_{ij}\]</span></p>
<p><span class="math display">\[\bar{Y}_{i..} = \frac{1}{bn}
\Sigma_{j=1}^b \Sigma_{k=1} ^{n} Y_{ijk} = \hat{\mu} +\hat \alpha_i =
\hat \mu_{i.}\]</span></p>
<p><span class="math display">\[\bar{Y}_{.j.} = \frac{1}{an}
\Sigma_{i=1}^a \Sigma_{k=1} ^{n} Y_{ijk} = \hat{\mu} +\hat \beta_j =
\hat \mu_{.j}\]</span></p>
<p><span class="math display">\[\bar{Y}_{...} =
\frac{1}{abn}\Sigma_{i=1}^a \Sigma_{j=1}^b \Sigma_{k=1} ^{n} Y_{ijk} =
\hat{\mu} \]</span></p>
<p>Two factors ANOVA 的方差分解更复杂一些：</p>
<p><span class="math display">\[\begin{aligned} SSTO &amp;= \Sigma_i
\Sigma_j \Sigma_k (Y_{ijk} - \bar Y_{...})^2   \\ &amp;=  \Sigma_i
\Sigma_j \Sigma_k ((\bar Y_{i..} - \bar Y_{...})+(\bar{Y}_{.j.} - \bar
Y_{...}) +(\bar{Y}_{ij.} - \bar Y_{i..} -\bar Y_{.j.} +\bar Y_{...})
+(Y_{ijk} - \bar Y_{ij.}))^2 \\&amp;=bn\Sigma_i (\bar Y_{i..} - \bar
Y_{...})^2+an\Sigma_j (\bar{Y}_{.j.} - \bar Y_{...})^2+n\Sigma_i
\Sigma_j (\bar{Y}_{ij.} - \bar Y_{i..} -\bar Y_{.j.} +\bar Y_{...})^2+
\Sigma_{i}\Sigma_j \Sigma_k (Y_{ijk} - \bar Y_{ij.})^2  \\&amp;=SSA
+SSB+SSAB+SSE  \end{aligned}\]</span></p>
<p>其中，<span class="math inline">\(SSA\)</span> 的自由度是 <span class="math inline">\(a-1\)</span>，<span class="math inline">\(SSB\)</span> 的自由度是 <span class="math inline">\(b-1\)</span>，<span class="math inline">\(SSAB\)</span> 的自由度是 <span class="math inline">\((a-1)(b-1)\)</span>，<span class="math inline">\(SSE\)</span> 的自由度是 <span class="math inline">\(ab(n-1)\)</span>，<span class="math inline">\(SSTO\)</span> 的自由度是 <span class="math inline">\(abn-1\)</span>。在这一个复杂问题中我们关心不同的问题，可以做出三种不同的假设检验，有对应的检验统计量。</p>
<ul>
<li><span class="math inline">\(A\)</span> 因素是否会导致差异？<span class="math inline">\(H_0: \alpha_1 =\alpha_2=...=\alpha_a\)</span>，在
<span class="math inline">\(H_0\)</span> 下有 <span class="math inline">\(F^* = \frac{SSA/(a-1)}{SSE/(ab(n-1))} \sim
F_{a-1,ab(n-1)}\)</span></li>
<li><span class="math inline">\(B\)</span> 因素是否会导致差异？<span class="math inline">\(H_0: \beta_1 =\beta_2=...=\beta_b\)</span>，在
<span class="math inline">\(H_0\)</span> 下有 <span class="math inline">\(F^* = \frac{SSB/(b-1)}{SSE/(ab(n-1))} \sim
F_{b-1,ab(n-1)}\)</span></li>
<li><span class="math inline">\(A,B\)</span> 是否联合作用？<span class="math inline">\(H_0:(\alpha \beta)_{ij}=0, \forall 1 \leq i \leq
a,1\leq j \leq b\)</span>，在 <span class="math inline">\(H_0\)</span>
下有 <span class="math inline">\(F^*=\frac{SSAB/(a-1)(b-1)}{SSE/(ab(n-1))} \sim
F_{(a-1)(b-1),ab(n-1)}\)</span></li>
</ul>
<p>不能直接用 chi-square 统计量作为检验统计量的原因是实际上 <span class="math inline">\(MSE,MSA\)</span> 等统计量中都带有未知的 <span class="math inline">\(\sigma^2\)</span> 参数项。</p>
<p>注意 <span class="math inline">\(n_{ij}\)</span>
是相等的也就是平衡设计，所以 Type I 和 Type III ANOVA
的结果是一样的。</p>
<h3 id="two-way-anova-in-r">Two-Way ANOVA in R</h3>
<p>模型假设是 <span class="math inline">\(Y_{ijk} = \mu +\alpha_i +
\beta_j +(\alpha \beta)_{ij} + \varepsilon_{ijk}\)</span>。</p>
<p>其中，<span class="math inline">\(\mu\)</span> 是整体的均值，<span class="math inline">\(\alpha_i\)</span> 代表只和等级为 <span class="math inline">\(i\)</span> 的因素 <span class="math inline">\(A\)</span> 有关的变化，<span class="math inline">\(\beta_j\)</span> 表示只和等级为 <span class="math inline">\(j\)</span> 的因素 <span class="math inline">\(B\)</span> 有关的变化，<span class="math inline">\((\alpha \beta)_{ij}\)</span>
表示和两个因素同时相关的变化，相当于一个交叉项。</p>
<p>这里面有 <span class="math inline">\((a+1)(b+1)\)</span>
个参数，对其也有一定的约束。R code 中的约束是和上述理论不同的，因此
estimator 的读取也并不相同，认为 <span class="math inline">\(\alpha_1 =
\beta_1 = (\alpha \beta)_{1j} = (\alpha
\beta)_{i1}=0\)</span>，事实上也还是 <span class="math inline">\(a+b+1\)</span> 个约束条件，可以估计出 <span class="math inline">\(ab+1\)</span> 个不同参数。</p>
<p>相应地，对应的 design matrix 也不尽相同。这里以 <span class="math inline">\(a=3,b=2,n=2\)</span> 为例，coefficient table
如下所示：</p>
<p><img src="https://s2.loli.net/2023/06/02/tjVnAgmPvwBoeQF.png" alt="coefficient-table.png"></p>
<p>Coefficient table 里的 Intercept 代表的实际上是 <span class="math inline">\(\mu\)</span>，预设了 <span class="math inline">\(\beta_1=0\)</span>，height2 即为 <span class="math inline">\(\beta_2\)</span>，height3 即为 <span class="math inline">\(\beta_3\)</span>；预设了 <span class="math inline">\(\alpha_1=0\)</span>，width2 即为 <span class="math inline">\(\alpha_2\)</span>；关于交互效应项，由于预设了
<span class="math inline">\((\alpha \beta)_{11} = (\alpha \beta)_{12} =
(\alpha \beta)_{13} =(\alpha
\beta)_{21}=0\)</span>，不为零的交互效应项只有两个，分别由
height2:weight2 对应 <span class="math inline">\((\alpha
\beta)_{22}\)</span>，height2:weight3 对应 <span class="math inline">\((\alpha \beta)_{23}\)</span>。</p>
<p>由此我们可以依次按照 <span class="math inline">\(\mu_{ij} = \mu +
\alpha_i + \beta_j +(\alpha \beta)_{ij}\)</span> 算出所有的 <span class="math inline">\(\mu_{ij}\)</span> 的估计量。</p>
<p>在这之后我们希望得到一个关于 <span class="math inline">\(12\)</span>
个数据的设计阵，实际上就是把数据和上述分析对应起来：</p>
<p><img src="https://s2.loli.net/2023/06/02/bAZXemT5kDncjoy.png" alt="design-matrix.png"></p>
<h2 id="least-square-means">Least Square Means</h2>
<p>遇到非平衡设计，或者 covariates
的情况（连续型变量和类别性变量产生交互效应的情况），需要考虑 least
square means 而不是 pooled means。简单来说 least square means
是均值的均值，达到了最小的方差，而 pooled means
就是全体数据的均值，在非平衡设计的情况下很可能引入偏差。</p>
<p><img src="https://s2.loli.net/2023/06/02/RP89DuvkNQlVUqi.png" alt="LSM.png"></p>
<h3 id="balanced-test">Balanced Test</h3>
<p>对于一个平衡设计来说，它本质上是一个正交设计，模型中进一个因子还是两个因子都不会改变彼此的系数估计，只有自由度会有改变。least
square mean 就是普通的 pooled mean，one-way ANOVA 和 two-way ANOVA
的结果一致。</p>
<h3 id="unbalanced-test">Unbalanced Test</h3>
<p>非平衡设计的时候 one-way ANOVA 的结果仍然是一样的，least square mean
和 pooled mean 得到的都是这一组内所有的观测值的平均。在这里就是认为 A
因子的 height1 level 的估计值是 <span class="math inline">\(\hat \mu_{1}
= \frac 1 3(X_{11} +X_{12} +X_{13}) = 43\)</span>。</p>
<p>但是新加入一个因子做 two-way ANOVA 就会导致 least square mean
下的系数估计发生变化，从 coefficient table 可以看出 <span class="math inline">\(\mu_{i1}\)</span> 和 <span class="math inline">\(\mu_{i2}\)</span>
都改变了（绷不住了，虽然并不知道是怎么变的，但是会从系数表读出每一个
<span class="math inline">\(\mu_{ij}\)</span> 就可以了），仍然使用 <span class="math inline">\(\frac 1 2(\mu_{i1}+\mu_{i2})\)</span> 作为 <span class="math inline">\(\mu_{i.}\)</span>
的估计，就也会相应地发生变化。</p>
<p>事实上 least square mean 还可以称作 predicted mean 的原因就是，此处的
<span class="math inline">\(\mu_{11}\)</span> 和 <span class="math inline">\(\mu_{12}\)</span> 分别作为 <span class="math inline">\(1\times 1,1\times 2\)</span>
这两格的预测值出现，它们的平均就作为 <span class="math inline">\(\mu_{1.}\)</span> 这一个 A 因子的 height1 level
对应的预测值出现。在具体的例子里，此处因为得到的估计是 <span class="math inline">\(\hat \mu_{11} = 41, \hat \mu_{12} =
44\)</span>，于是认为 <span class="math inline">\(\hat \mu_{1.} = \frac
1 2 (\hat \mu_{11} + \hat \mu_{12}) = 42.5\)</span>。</p>
<p><img src="https://s2.loli.net/2023/06/02/McpX95nUsHYWRmT.png" alt="predicted-means.png"></p>
<p>总之可以看到，unbalance test 会导致实验设计并不是正交的，B
因子的加入会对 A 因子的均值估计产生影响，这也是我们一般希望使用 balanced
test 的理由。</p>
<h2 id="model-diagnose-remedy">Model Diagnose &amp; Remedy</h2>
<p>马上要告别应统了，于是不考也就不想学了，摆烂。</p>
<h1 id="附录">附录</h1>
<h2 id="常见重要分布">常见重要分布</h2>
<p>From <a target="_blank" rel="noopener" href="https://v1ncent19.github.io/SummaryNotes/">Statistic
Note</a> P10, by V1ncent19</p>
<table style="width:100%;">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(X\)</span></th>
<th><span class="math inline">\(p_X(k)\big/f_X(x)\)</span></th>
<th><span class="math inline">\(\mathbb{E}\)</span></th>
<th><span class="math inline">\(var\)</span></th>
<th>PGF</th>
<th>MGF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mathrm{Bern} (p)\)</span></td>
<td></td>
<td><span class="math inline">\(p\)</span></td>
<td><span class="math inline">\(pq\)</span></td>
<td></td>
<td><span class="math inline">\(q+pe^s\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(B (n,p)\)</span></td>
<td><span class="math inline">\(C_n^k p^k(1-p)^{n-k}\)</span></td>
<td><span class="math inline">\(np\)</span></td>
<td><span class="math inline">\(npq\)</span></td>
<td><span class="math inline">\((q+ps)^n\)</span></td>
<td><span class="math inline">\((q+pe^s)^n\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathrm{Geo} (p)\)</span></td>
<td><span class="math inline">\((1-p)^{k-1}p\)</span></td>
<td><span class="math inline">\(\dfrac{1}{p}\)</span></td>
<td><span class="math inline">\(\dfrac{q}{p^2}\)</span></td>
<td><span class="math inline">\(\dfrac{ps}{1-qs}\)</span></td>
<td><span class="math inline">\(\dfrac{pe^s}{1-qe^s}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(H(n,M,N)\)</span></td>
<td><span class="math inline">\(\dfrac{C_M^kC_{N-M}^{n-k}}{C_N^n}\)</span></td>
<td><span class="math inline">\(n\dfrac{M}{N}\)</span></td>
<td><span class="math inline">\(\dfrac{nM(N-n)(N-M)}{N^2(n-1)}\)</span></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(P(\lambda)\)</span></td>
<td><span class="math inline">\(\dfrac{\lambda^k}{k!}e^{-\lambda}\)</span></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(e^{\lambda(s-1)}\)</span></td>
<td><span class="math inline">\(e^{\lambda(e^s-1)}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(U(a,b)\)</span></td>
<td><span class="math inline">\(\dfrac{1}{b-a}\)</span></td>
<td><span class="math inline">\(\dfrac{a+b}{2}\)</span></td>
<td><span class="math inline">\(\dfrac{(b-a)^2}{12}\)</span></td>
<td></td>
<td><span class="math inline">\(\dfrac{e^{sb}-e^{sa}}{(b-a)^s}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(N(\mu,\sigma^2)\)</span></td>
<td><span class="math inline">\(\dfrac{1}{\sigma
\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)</span></td>
<td><span class="math inline">\(\mu\)</span></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
<td></td>
<td><span class="math inline">\(e^{\frac{\sigma^2s^2}{2}+\mu
s}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\epsilon(\lambda)\)</span></td>
<td><span class="math inline">\(\lambda e^{-\lambda x}\)</span></td>
<td><span class="math inline">\(\dfrac{1}{\lambda}\)</span></td>
<td><span class="math inline">\(\dfrac{1}{\lambda^2}\)</span></td>
<td></td>
<td><span class="math inline">\(\frac{\lambda}{\lambda-s}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\Gamma(\alpha,\lambda)\)</span></td>
<td><span class="math inline">\(\dfrac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda
x}\)</span></td>
<td><span class="math inline">\(\dfrac{\alpha}{\lambda}\)</span></td>
<td><span class="math inline">\(\dfrac{\alpha}{\lambda^2}\)</span></td>
<td></td>
<td><span class="math inline">\(\left(\frac{\lambda}{\lambda-s}\right)^\alpha\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(B(\alpha,\beta)\)</span></td>
<td><span class="math inline">\(\dfrac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}\)</span></td>
<td><span class="math inline">\(\dfrac{\alpha}{\alpha+\beta}\)</span></td>
<td><span class="math inline">\(\dfrac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\)</span></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\chi^2_n\)</span></td>
<td><span class="math inline">\(\dfrac{1}{2^{\frac{n}{2}}\Gamma(\frac{n}{2})}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}\)</span></td>
<td><span class="math inline">\(n\)</span></td>
<td><span class="math inline">\(2n\)</span></td>
<td></td>
<td>$ (1-2s)^{-n/2} $</td>
</tr>
<tr class="even">
<td><span class="math inline">\(t_\nu\)</span></td>
<td><span class="math inline">\(\dfrac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}(1+\frac{x^2}{\nu})^{-\frac{\nu+1}{2}}\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(\dfrac{\nu}{\nu-2}\)</span></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(F_{m,n}\)</span></td>
<td><span class="math inline">\(\dfrac{\Gamma(\frac{m+n}{2})}{\Gamma(\frac{m}{2})\Gamma(\frac{n}{2})}\dfrac{m^\frac{m}{2}n^\frac{n}{2}x^{\frac{m}{2}-1}}{(mx+n)^{\frac{m+n}{2}}}\)</span></td>
<td><span class="math inline">\(\dfrac{n}{n-2}\)</span></td>
<td><span class="math inline">\(\dfrac{2n^2(m+n-2)}{m(n-2)^2(n-4)}\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Consider <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span>
i.i.d. <span class="math inline">\(\sim N(0,1)\)</span>; <span class="math inline">\(Y,Y_1,Y_2,\ldots,Y_m\)</span> i.i.d. <span class="math inline">\(\sim N(0,1)\)</span> - <span class="math inline">\(\chi^2\)</span> Distribution：</p>
<p><span class="math inline">\(\chi^2\)</span> distribution with degree
of freedom <span class="math inline">\(n\)</span>：$ =_{i=1}^n
X_i<sup>2</sup>2_n$。 For independent <span class="math inline">\(\xi_i\sim\chi^2_{n_i},\,
i=1,2,\ldots,k\)</span>：<span class="math inline">\(x_{i_0}=\sum_{i=1}^k\xi_i\sim\chi^2_{n_1+\ldots+n_k}\)</span></p>
<ul>
<li><p><span class="math inline">\(t\)</span> Distribution：</p>
<p><span class="math inline">\(t\)</span> distribution with degree of
freedom <span class="math inline">\(n\)</span>：$ T==t_n$</p>
<p>Upper <span class="math inline">\(\alpha\)</span>-fractile of <span class="math inline">\(t_\nu\)</span>, satisfies <span class="math inline">\(\mathbb{P}(T\geq
c)=\alpha\)</span>，$t_{,}=<em>{c}(Tc)=,Tt</em>$</p></li>
<li><p><span class="math inline">\(F\)</span> Distribution：</p>
<p><span class="math inline">\(F\)</span> distribution with degree of
freedom <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>：$ F=F_{m,n}$</p>
<ul>
<li>If <span class="math inline">\(Z\sim F_{m,n}\)</span>, then <span class="math inline">\(\dfrac{1}{Z}\sim F_{n,m}\)</span>；</li>
<li>If <span class="math inline">\(T\sim t_n\)</span>, then <span class="math inline">\(T^2\sim F_{1,n}\)</span>；</li>
<li><span class="math inline">\(F_{m,n,1-\alpha}=\dfrac{1}{F_{n,m,\alpha}}\)</span>
。</li>
</ul></li>
</ul>
<p>Some useful lemmas in statistical inference：</p>
<ul>
<li><p>For <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span>
independent with <span class="math inline">\(X_i\sim
N(\mu_i,\sigma^2_i)\)</span>, then <span class="math inline">\(\sum_{i=1}^n\left(\frac{X_i-\mu_i}{\sigma_i}\right)^2\sim
\chi^2_n\)</span></p></li>
<li><p>For <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span>
i.i.d.<span class="math inline">\(\sim N(\mu,\sigma^2)\)</span>, then $
T=t_{n-1} $</p></li>
<li><p>For <span class="math inline">\(X_1,X_2,\ldots,X_m\)</span>
i.i.d.<span class="math inline">\(\sim N(\mu_1,\sigma^2)\)</span>, <span class="math inline">\(Y_1,Y_2,\ldots,Y_n\)</span> i.i.d.<span class="math inline">\(\sim N(\mu_2,\sigma^2)\)</span>,d enote sample
pooled variance <span class="math inline">\(S_{\omega}^2=\dfrac{(m-1)S^2_1+(n-1)S^2_2}{m+n-2}\)</span>,
then <span class="math inline">\(T=\frac{(\bar{X}-\bar{Y})-(\mu_1-\mu_2)}{S_{\omega}}\cdot
\sqrt{\frac{mn}{m+n}}\sim t_{m+n-2}\)</span></p></li>
<li><p>For <span class="math inline">\(X_1,X_2,\ldots,X_m\)</span>
i.i.d.<span class="math inline">\(\sim N(\mu,\sigma^2)\)</span>, <span class="math inline">\(Y_1,Y_2,\ldots,Y_n\)</span> i.i.d.<span class="math inline">\(\sim N(\mu_2,\sigma^2)\)</span>, then
$T=F_{m-1,n-1} $</p></li>
<li><p>For <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span>
i.i.d. <span class="math inline">\(\sim \varepsilon(\lambda)\)</span>,
then $ 2n{X}=2<em>{i=1}^nX_i ^2</em>{2n} $</p>
<p>Remark：for <span class="math inline">\(X_i\sim\varepsilon(\lambda)=\Gamma(1,\lambda)\)</span>，<span class="math inline">\(2\lambda\sum_{i=1}^nX_i\sim\Gamma(n,1/2)=\chi^2_{2n}\)</span>.</p></li>
</ul>
<h2 id="分位数速查">分位数速查</h2>
<p>来自 Package stats, version 4.2.1。</p>
<p>其实都可以直接查文档啦（，R 的文档还是很保姆式的。</p>
<h3 id="t-分布模拟">t-分布模拟</h3>
<p>t-distribution 下有四个函数，分别是 density, CDF, quantile
function(<span class="math inline">\(CDF^{-1}\)</span>)，还有一个是随机生成一个模拟数组。</p>
<p><code>dt(x, df, ncp, log = FALSE)</code> 用来计算 PDF 的函数值 <span class="math inline">\(f(x)\)</span>，df 是自由度，ncp 表示非中心化参数
<span class="math inline">\(\delta\)</span>；</p>
<p><code>pt(x, df, ncp, lower.tail = TRUE, log.p = FALSE)</code>
用来计算 CDF 的函数值 <span class="math inline">\(F(x)\)</span>，注意
<code>lower.tail = TRUE</code> 时计算的是左边值 <span class="math inline">\(F(x)\)</span>，否则实际计算了 <span class="math inline">\(1-F(x)\)</span>。</p>
<p><code>qt(p, df, ncp, lower.tail = TRUE, log.p = TRUE)</code>
用来计算分位数，也即 <span class="math inline">\(F^{-1}(p)\)</span>，其他参数意义同上。</p>
<p>一些我的作业里的函数参考：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> qt<span class="punctuation">(</span><span class="number">1</span> <span class="operator">-</span> <span class="number">0.025</span><span class="punctuation">,</span> df <span class="operator">=</span> <span class="number">8</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">2.306004</span></span><br></pre></td></tr></table></figure>
<p>这里计算的是 <span class="math inline">\(t_{8,0.975}\)</span>
的下分位数，实际上是一个 level of significance 为 <span class="math inline">\(0.05\)</span> 的双尾检验中用到的分位数。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> <span class="number">2</span> <span class="operator">*</span> pt<span class="punctuation">(</span><span class="operator">-</span><span class="number">8.529</span><span class="punctuation">,</span> df <span class="operator">=</span> <span class="number">8</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">2.74639e-05</span></span><br></pre></td></tr></table></figure>
<p>这里是在计算一个 P-value，计算的是比 observed data <span class="math inline">\(-8.529\)</span> 更极端的数据的出现概率，单边是
<span class="math inline">\(F (-8.529)\)</span>，注意此处的“极端”包含比
<span class="math inline">\(-8.529\)</span> 更小和比 <span class="math inline">\(8.529\)</span> 更大这两种情况，实际上是 <span class="math inline">\(F(-8.529) + 1- F(8.529)\)</span>。</p>
<p>由于 t-distribution 是对称的，可以简化为 <span class="math inline">\(2 \times F(-8.529)\)</span>。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> ncp <span class="operator">&lt;-</span> 2.0<span class="operator">/</span><span class="number">0.50</span></span><br><span class="line"><span class="operator">&gt;</span> <span class="built_in">c</span> <span class="operator">&lt;-</span> qt<span class="punctuation">(</span><span class="number">1</span><span class="operator">-</span><span class="number">0.025</span><span class="punctuation">,</span>df<span class="operator">=</span><span class="number">8</span><span class="punctuation">)</span></span><br><span class="line"><span class="operator">&gt;</span> pt<span class="punctuation">(</span><span class="operator">-</span><span class="built_in">c</span><span class="punctuation">,</span> df<span class="operator">=</span><span class="number">8</span><span class="punctuation">,</span> ncp<span class="punctuation">)</span> <span class="operator">+</span> <span class="number">1</span> <span class="operator">-</span> pt<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">,</span> df<span class="operator">=</span><span class="number">8</span><span class="punctuation">,</span> ncp<span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">0.9367429</span></span><br></pre></td></tr></table></figure>
<p>这是一个非中心的 t 检验，实际上是在计算 Power Function。</p>
<h3 id="正态分布模拟">正态分布模拟</h3>
<p>常用的三个函数是
<code>dnorm</code>，<code>pnorm</code>，<code>qnorm</code>，含义与
t-分布中的 <code>dt</code>，<code>pt</code>，<code>qt</code> 相似。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dnorm<span class="punctuation">(</span>x<span class="punctuation">,</span> mean <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span> sd <span class="operator">=</span> <span class="number">1</span><span class="punctuation">,</span> <span class="built_in">log</span> <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">)</span></span><br><span class="line">pnorm<span class="punctuation">(</span>q<span class="punctuation">,</span> mean <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span> sd <span class="operator">=</span> <span class="number">1</span><span class="punctuation">,</span> lower.tail <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span> log.p <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">)</span></span><br><span class="line">qnorm<span class="punctuation">(</span>p<span class="punctuation">,</span> mean <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span> sd <span class="operator">=</span> <span class="number">1</span><span class="punctuation">,</span> lower.tail <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span> log.p <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>注意一般正态检验中都是使用标准正态分布，也就是不需要去改变
<code>mean</code> 和 <code>sd</code> 的默认值。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> pnorm<span class="punctuation">(</span><span class="number">0</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">0.5</span></span><br><span class="line"><span class="operator">&gt;</span> qnorm<span class="punctuation">(</span><span class="number">0.025</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">-</span><span class="number">1.959964</span></span><br></pre></td></tr></table></figure>
<h3 id="f-分布模拟">F-分布模拟</h3>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df<span class="punctuation">(</span>x<span class="punctuation">,</span> df1<span class="punctuation">,</span> df2<span class="punctuation">,</span> ncp<span class="punctuation">,</span> <span class="built_in">log</span> <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">)</span></span><br><span class="line">pf<span class="punctuation">(</span>q<span class="punctuation">,</span> df1<span class="punctuation">,</span> df2<span class="punctuation">,</span> ncp<span class="punctuation">,</span> lower.tail <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span> log.p <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">)</span></span><br><span class="line">qf<span class="punctuation">(</span>p<span class="punctuation">,</span> df1<span class="punctuation">,</span> df2<span class="punctuation">,</span> ncp<span class="punctuation">,</span> lower.tail <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span> log.p <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>注意 <span class="math inline">\(F\)</span>
分布的两个自由度都可以取到无穷，写作 <code>df1 = Inf</code>。</p>
<h3 id="chi-square-分布模拟">Chi-square 分布模拟</h3>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dchisq<span class="punctuation">(</span>x<span class="punctuation">,</span> df<span class="punctuation">,</span> ncp <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span> <span class="built_in">log</span> <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">)</span></span><br><span class="line">pchisq<span class="punctuation">(</span>q<span class="punctuation">,</span> df<span class="punctuation">,</span> ncp <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span> lower.tail <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span> log.p <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">)</span></span><br><span class="line">qchisq<span class="punctuation">(</span>p<span class="punctuation">,</span> df<span class="punctuation">,</span> ncp <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span> lower.tail <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span> log.p <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<h2 id="其他常用-r-命令">其他常用 R 命令</h2>
<p>持续更新中，基本都是作业里扒出来的。</p>
<h3 id="confidence-interval">confidence interval</h3>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confint<span class="punctuation">(</span>object<span class="punctuation">,</span> parm<span class="punctuation">,</span> level <span class="operator">=</span> <span class="number">0.95</span><span class="punctuation">,</span> ...<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>注意 confidence coefficient 的默认值是 <span class="math inline">\(0.95\)</span>，<code>confint</code>
函数是用于拟合模型参数的置信区间估计，例如：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model <span class="operator">&lt;-</span> lm<span class="punctuation">(</span>data<span class="punctuation">)</span></span><br><span class="line">confint<span class="punctuation">(</span>model<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">               <span class="number">2.5</span> <span class="operator">%    97.5 %</span></span><br><span class="line"><span class="punctuation">(</span>Intercept<span class="punctuation">)</span> <span class="number">8.670370</span> <span class="number">11.729630</span></span><br><span class="line">V2          <span class="number">2.918388</span>  <span class="number">5.081612</span></span><br></pre></td></tr></table></figure>
<p>这个附录怎么全咕了啊，不过无所谓了，这课我本来就是在摆烂（</p>
<h1 id="完结撒花">完结撒花</h1>
<p>一点都不 happily ever
after，说是找到了新的方向，谁知道概率又会不会很艰难呢。</p>
<p>这课明显东西比统计推断多，但是导出成 PDF
一看比统推笔记少了二十多页。</p>
<p>总之都结束了，笑一个吧（</p>
<p><img src="https://s2.loli.net/2022/12/21/6TEM1vSosXLcwOg.jpg" alt="HEA.jpg"></p>

    </div>

    
    
    
        <div class="reward-container">
  <div>我很可爱 请给我钱（？）</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="驰雨Chiyuru 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="驰雨Chiyuru 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>驰雨Chiyuru
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://chiyuru.github.io/2023/02/21/Linear-Regression-Analysis/" title="还就那个线性回归分析">https://chiyuru.github.io/2023/02/21/Linear-Regression-Analysis/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%AF%BE%E7%A8%8B%E5%AE%9E%E5%BD%95/" rel="tag"># 课程实录</a>
              <a href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag"># 数学</a>
              <a href="/tags/%E7%BB%9F%E8%AE%A1/" rel="tag"># 统计</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/02/01/Learn-Mathematical-Analysis-or-Die-2/" rel="prev" title="数学分析 I 速通日志（下）">
      <i class="fa fa-chevron-left"></i> 数学分析 I 速通日志（下）
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/02/25/Multivariate-Statistical-Analysis/" rel="next" title="多元问题对我来说太难了，但是多元统计分析">
      多元问题对我来说太难了，但是多元统计分析 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  
  <div class="comments">
  <script src="https://utteranc.es/client.js" repo="Chiyuru/chiyuru.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async></script>
  </div>
  
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-1"><span class="nav-text">Lecture 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#galtons-experiment"><span class="nav-text">Galton&#39;s Experiment</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E6%9C%AF%E8%AF%AD"><span class="nav-text">一些术语</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-2"><span class="nav-text">Lecture 2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#simple-linear-regression"><span class="nav-text">Simple Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%9A%84%E7%BB%84%E7%BB%87%E5%92%8C%E8%A1%A8%E7%A4%BA"><span class="nav-text">数据的组织和表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A1%A8%E7%A4%BA"><span class="nav-text">模型的表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E7%9A%84%E6%84%8F%E4%B9%89%E5%92%8C%E6%B1%82%E7%AE%97"><span class="nav-text">参数的意义和求算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#least-sum-of-square-%E6%96%B9%E6%B3%95"><span class="nav-text">Least Sum of Square 方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mle-%E6%96%B9%E6%B3%95"><span class="nav-text">MLE 方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E6%96%AD%E5%A4%8D%E4%B9%A0"><span class="nav-text">推断复习</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-3"><span class="nav-text">Lecture 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E6%8E%A8%E6%96%AD"><span class="nav-text">线性回归中的推断</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E6%8E%A8%E6%96%AD"><span class="nav-text">参数推断</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#power-function"><span class="nav-text">Power Function</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#blue"><span class="nav-text">BLUE</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-4"><span class="nav-text">Lecture 4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#estimation-prediction"><span class="nav-text">Estimation &amp; Prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B3%E5%9D%87%E5%93%8D%E5%BA%94%E7%9A%84%E6%8E%A8%E6%96%AD"><span class="nav-text">平均响应的推断</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E5%80%BC%E7%9A%84%E6%8E%A8%E6%96%AD"><span class="nav-text">预测值的推断</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E5%80%BC%E7%9A%84%E5%B9%B3%E5%9D%87%E7%9A%84%E6%8E%A8%E6%96%AD"><span class="nav-text">预测值的平均的推断</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#confidence-band-for-entire-regression-line"><span class="nav-text">Confidence Band for
Entire Regression Line</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary"><span class="nav-text">Summary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#analysis-of-variance-anova"><span class="nav-text">Analysis of Variance (ANOVA)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#variance-estimator"><span class="nav-text">Variance Estimator</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#f-%E6%A3%80%E9%AA%8C"><span class="nav-text">F-检验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#general-linear-test"><span class="nav-text">General Linear Test</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pearson-correlation-r"><span class="nav-text">Pearson Correlation r</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-5"><span class="nav-text">Lecture 5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#diagnostics-of-x"><span class="nav-text">Diagnostics of X</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#why-diagnosedistribution-and-confounding"><span class="nav-text">Why
diagnose——Distribution and Confounding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E5%8F%82%E6%95%B0"><span class="nav-text">四参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%8A%E6%96%AD-assumptions"><span class="nav-text">诊断 assumptions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB"><span class="nav-text">非线性关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%82%E6%96%B9%E5%B7%AE%E9%97%AE%E9%A2%98"><span class="nav-text">异方差问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E6%80%81%E6%80%A7%E5%81%87%E8%AE%BE"><span class="nav-text">正态性假设</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E6%80%A7"><span class="nav-text">相关性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#outlier"><span class="nav-text">Outlier</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E-r2"><span class="nav-text">关于 \(R^2\)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-6"><span class="nav-text">Lecture 6</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#built-in-diagnostic-plots-in-r"><span class="nav-text">Built-in Diagnostic Plots in
R</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#scale-location-plot"><span class="nav-text">Scale-Location Plot</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cooks-distance"><span class="nav-text">Cook&#39;s Distance</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#residuals-leverage"><span class="nav-text">Residuals &amp; Leverage</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#leverage"><span class="nav-text">Leverage</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#studentized-residual"><span class="nav-text">Studentized Residual</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cooks-distance-1"><span class="nav-text">Cook&#39;s Distance</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lack-of-fit-test"><span class="nav-text">Lack of fit test</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#remedy-methods"><span class="nav-text">Remedy Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%A5%E6%95%91%E9%9D%9E%E7%BA%BF%E6%80%A7"><span class="nav-text">补救非线性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%A5%E6%95%91%E5%BC%82%E6%96%B9%E5%B7%AE"><span class="nav-text">补救异方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%A5%E6%95%91%E9%9D%9E%E6%AD%A3%E6%80%81"><span class="nav-text">补救非正态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transformation"><span class="nav-text">Transformation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#miscellaneous-topics"><span class="nav-text">Miscellaneous Topics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#regression-through-the-origin"><span class="nav-text">Regression Through the
Origin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#inverse-predictions"><span class="nav-text">Inverse Predictions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#limitations-of-r2"><span class="nav-text">Limitations of \(R^2\)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-7"><span class="nav-text">Lecture 7</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E8%A1%A8%E8%BE%BE"><span class="nav-text">矩阵表达</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hat-matrix"><span class="nav-text">Hat Matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multiple-linear-regression"><span class="nav-text">Multiple Linear Regression</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-8"><span class="nav-text">Lecture 8</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#explanatory-data-analysistransformation"><span class="nav-text">Explanatory Data
Analysis——Transformation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#why-look-at-y"><span class="nav-text">Why look at Y</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-log-transformation"><span class="nav-text">Why log Transformation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mlr-vs-slr"><span class="nav-text">MLR vs SLR</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%98%E9%87%8F%E9%80%89%E6%8B%A9"><span class="nav-text">变量选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%A1%E5%BC%82%E7%9A%84%E7%8E%B0%E8%B1%A1"><span class="nav-text">诡异的现象</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#inference"><span class="nav-text">Inference</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-9"><span class="nav-text">Lecture 9</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#extra-sum-of-squares"><span class="nav-text">Extra Sum of Squares</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#general-linear-test-1"><span class="nav-text">General Linear Test</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#test-reduced-model"><span class="nav-text">Test Reduced Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#test-linear-hypothesis"><span class="nav-text">Test Linear Hypothesis</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%81%8F%E5%86%B3%E5%AE%9A%E7%B3%BB%E6%95%B0-%E5%81%8F%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0"><span class="nav-text">偏决定系数 &amp; 偏相关系数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%9B%9E%E5%BD%92"><span class="nav-text">标准回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#motivation"><span class="nav-text">Motivation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#methodcorrelation-transformation"><span class="nav-text">Method——Correlation
Transformation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#suppressor-variable"><span class="nav-text">Suppressor Variable</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-10"><span class="nav-text">Lecture 10</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#multicollinearity"><span class="nav-text">Multicollinearity</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#zero-collinearity"><span class="nav-text">Zero Collinearity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#linearly-dependent"><span class="nav-text">Linearly Dependent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multicollinearity-1"><span class="nav-text">Multicollinearity</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#polynomial-regression"><span class="nav-text">Polynomial Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%A4%E4%BA%92%E9%A1%B9"><span class="nav-text">交互项</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-11"><span class="nav-text">Lecture 11</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95"><span class="nav-text">模型选择方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E6%B2%A1%E9%82%A3%E4%B9%88%E6%95%B0%E5%AD%A6%E7%9A%84"><span class="nav-text">一些没那么数学的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E5%87%86%E5%88%99"><span class="nav-text">一些准则</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%8A%E6%96%AD"><span class="nav-text">模型诊断</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#partial-regression-plots"><span class="nav-text">Partial Regression Plots</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#studentized-residuals"><span class="nav-text">Studentized Residuals</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#assessing-outliers"><span class="nav-text">Assessing Outliers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multicollinearity-diagnose"><span class="nav-text">Multicollinearity Diagnose</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-12"><span class="nav-text">Lecture 12</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#equal-variance-remedy"><span class="nav-text">Equal Variance Remedy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#weighted-regression"><span class="nav-text">Weighted Regression</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multicollinearity-remedy"><span class="nav-text">Multicollinearity Remedy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ridge-regression"><span class="nav-text">Ridge Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lasso-elastic-net"><span class="nav-text">LASSO &amp; Elastic Net</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#influencial-cases-remedy"><span class="nav-text">Influencial Cases Remedy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nonlinearity-remedy"><span class="nav-text">Nonlinearity Remedy</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-13"><span class="nav-text">Lecture 13</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#one-factor-anova"><span class="nav-text">One Factor ANOVA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cell-means-model"><span class="nav-text">Cell means model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#factor-effects-model"><span class="nav-text">Factor Effects Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example"><span class="nav-text">Example</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#inference-on-one-way-anova"><span class="nav-text">Inference on One-Way ANOVA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#confidence-interval-for-mu_i"><span class="nav-text">Confidence Interval for \(\mu_i\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bonferroni-confidence-intervals-for-mu_i"><span class="nav-text">Bonferroni Confidence
Intervals for \(\mu_i\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#test-difference-in-means"><span class="nav-text">Test Difference in Means</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#contrast"><span class="nav-text">Contrast</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#concept"><span class="nav-text">Concept</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multiple-contrasts"><span class="nav-text">Multiple Contrasts</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-14"><span class="nav-text">Lecture 14</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#two-way-anova"><span class="nav-text">Two-Way ANOVA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cell-mean-model"><span class="nav-text">Cell Mean Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#factor-effects-model-1"><span class="nav-text">Factor Effects Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#two-way-anova-in-r"><span class="nav-text">Two-Way ANOVA in R</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#least-square-means"><span class="nav-text">Least Square Means</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#balanced-test"><span class="nav-text">Balanced Test</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unbalanced-test"><span class="nav-text">Unbalanced Test</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#model-diagnose-remedy"><span class="nav-text">Model Diagnose &amp; Remedy</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%87%8D%E8%A6%81%E5%88%86%E5%B8%83"><span class="nav-text">常见重要分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E4%BD%8D%E6%95%B0%E9%80%9F%E6%9F%A5"><span class="nav-text">分位数速查</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#t-%E5%88%86%E5%B8%83%E6%A8%A1%E6%8B%9F"><span class="nav-text">t-分布模拟</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E6%A8%A1%E6%8B%9F"><span class="nav-text">正态分布模拟</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#f-%E5%88%86%E5%B8%83%E6%A8%A1%E6%8B%9F"><span class="nav-text">F-分布模拟</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#chi-square-%E5%88%86%E5%B8%83%E6%A8%A1%E6%8B%9F"><span class="nav-text">Chi-square 分布模拟</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E5%B8%B8%E7%94%A8-r-%E5%91%BD%E4%BB%A4"><span class="nav-text">其他常用 R 命令</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#confidence-interval"><span class="nav-text">confidence interval</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%8C%E7%BB%93%E6%92%92%E8%8A%B1"><span class="nav-text">完结撒花</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="驰雨Chiyuru"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">驰雨Chiyuru</p>
  <div class="site-description" itemprop="description">おはよう、朝だよ</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">46</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Chiyuru" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Chiyuru" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chiyuruu@gmail.com" title="E-Mail → mailto:chiyuruu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Chiyuru_0417" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;Chiyuru_0417" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/chiyuruu" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;chiyuruu" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>知乎</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<div class="copyright">
  
  &copy; 2022 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">驰雨Chiyuru</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv1">
  本站总访客数：<span id="busuanzi_value_site_uv"></span>
</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
